<div align=center STYLE="page-break-after: always;">
    <br/><br/><br/><br/><br/><br/><br/><br/>
<!--标题调整，在这里选择你想要的字号和字体-->
    <font size=12 face="黑体" >
        <strong>操作系统讲义<strong>
          <br/><br/>
         <font size=6 face="仿宋">
    北京师范大学   2023-2024秋季学期
     <br/>
     <br/>
    <img src="C:\Users\21117\Pictures\Typora\OIP.jpg">
    <br/>
         <br/>
    <font size=6 face="宋体">
       Bai_Yu


<div style="page-break-after:always"></div>

[TOC]



<div style="page-break-after:always"></div>

#  绪论：怎样学习操作系统？

​	怎样学习操作系统的问题，是操作系统学科中最核心的方法论问题。学习操作系统的过程，绝不是一蹴而就、一步登天的过程，而是一个从底层开始，建立系统认知，理解主要概念、完成结构搭建的过程。这个过程强调学习者的逻辑思维能力、信息处理能力、实践能力和自立自强能力。直白地说，学好操作系统，就是要了解五个关键要素及其之间的关系：进程、处理机、存储器，文件系统和输入输出系统。

​	学习操作系统，必须坚持以进程为主体。进程是操作系统中最重要、最根本、最直接的研究对象。研究进程，就是要研究对进程的控制、进程间的同步、进程间的通信过程。研究进程时，要充分认识其内部的信息存储、申请资源的性质、进程间的关系，进程状态转变的原因。

​	学习操作系统，必须坚持处理机为调度中心。调度是分配内存的过程。调度离不开处理机，没有不依赖处理机的调度。任何一种处理机都有多种属于自己的调度算法，任何一种调度方式都要听从处理机的指令。处理机的调度包含三个层次：作业调度、内存调度、进程调度。死锁是进程调度过程中躲不开、逃不掉的根本问题，与计算机系统的安全息息相关、密不可分。迪杰斯特拉先生针对什么是死锁、如何避免死锁、如何解决死锁问题发表了重要论述。他强调，死锁产生的根本原因就是资源不足。解除死锁，就是要释放资源、停止进程、延缓调度；避免死锁，就是要模拟分配、检查情况、确定分配。这一系列表述结晶成了死锁定理和银行家算法。死锁定理和银行家算法扫除了死锁问题在进程调度中的阴霾。从此，进程调度的前途畅通无阻，焕然一新。

​	学习操作系统，必须坚持存储器为调度单元。对存储器的研究要牢牢把握其局部性原理，必须时刻清醒地认识到其空间上和时间上的局部性特点已经深刻融入存储器上的算法设计、过程分析、管理方式。存储器的基本管理方式包括分页管理、分段管理和段页式组合管理。分页管理是一切管理方法的基础。这种管理方式创造性地将逻辑地址和物理地址分成等大小的页，从而实现地址的映射。分页存储管理方式的提出，是一次具有高创新性、高部署性、高实践性的存储器管理方式的飞跃。为后续的许多管理方式夯实了基础。

​	学习操作系统，必须坚持文件系统为管理对象。文件系统实现了操作系统对文件的集中管理、集中控制、集中调度，是我们碰得着、看得见的系统，对于操作系统的学习有重要的实践性意义。

​	学习操作系统，必须坚持输入输出系统为重要保障。输入输出系统是用户与计算机交互、计算机与计算机交互过程中不可或缺的系统。有了输入输出系统，我们才能以一个学习者、控制者、命令者的姿态指导计算机完成自我使命。

<div style="page-break-after:always"></div>



# 第一章  进程管理

## 1.1  进程的描述

​	进程，是操作系统中最重要的一个概念。这门课绝大部分的内容都是围绕着进程展开的。

​	**定义1.1.1（进程）** 进程（Process）是程序的一次运行，是程序及其数据在处理机上顺序执行时发生的活动。

​	进程和程序之间最本质的区别是：进程具有动态性，而程序只是静态的代码和数据。在后面的叙述中，当我们说到“运行”这一动作，我们一般不区分“进程”和“程序”。我们知道，对于一台计算机，每时每刻都会有许许多多的进程在运行，也就是说，有许许多多的程序在做指令的顺序执行。那么这些正在运行的程序之间是否存在什么关系呢？事实上，这种关系有三类情况：

1. 多个程序在处理器上严格地按照**顺序运行**。当且仅当前一个程序完成运行之后，后一程序才能开始运行。
2. 在同一时间段内，多个程序在处理机上**并发运行**，即这些程序在该时间段内都会运行（不是同时运行）。
3. 在同一时刻，多个程序在处理机上**并行运行**，即这些程序同时运行。

​	由于并行运行涉及到处理机与操作系统的良好性质，我们在操作系统中一般不作讨论。接下来我们着重讨论一下程序的顺序运行和并发运行的情况。为了形象化地理解程序的顺序运行，我们引入**前趋图**的概念：

​	**定义1.1.2（前趋图）** 一个用于描述进程之间运行的前后关系的有向不循环图。其中，箭头指向的方向为程序的顺序运行方向。如下图中的 $(b)$ 就显然不是前趋图。

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231123183811791.png" alt="image-20231123183811791" style="zoom: 33%;" />

​	例如，对于上面的 $(a)$ 图，显然存在如下前趋关系：
$$
P1→P2, P1→P3, P1→P4, P2→P5, P3→P5, P4→P6, \\
P4→P7, P5→P8, P6→P8, P7→P9, P8→P9
$$
可以表示为：
$$
P=\{ (P1, P2), (P1, P3), (P1, P4), (P2, P5), (P3, P5), (P4, P6), \\
(P4, P7),(P5, P8), (P6, P8), (P7, P9), (P8, P9)\}
$$
又例如，用实际的程序来说，“输入 $\rightarrow$ 计算 $\rightarrow$ 打印”三个步骤可能具有这样的前趋图：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-11-23 184706.png" alt="屏幕截图 2023-11-23 184706" style="zoom:33%;" />

​	在程序并发运行时，可能存在以下特点：**间断性**、**失去封闭性**、**不可再现性**。**间断性**是指程序并发运行时，可能出现多个程序之间存在相互制约关系而导致某个程序出现运行 $\rightarrow$ 暂停 $\rightarrow$ 运行的过程；**失去封闭性**是指程序运行时所使用的资源可能会受外界的影响；**不可再现性**是指程序多次运行可能导致不一样的结果。

​	我们使用一个例子来说明不可再现性：在下图中，变量 X 为共享变量，程序 1 和程序 2 都要实现 X + 1 的操作。当两个程序运行的速度变化时可得到不同的结果。

~~~
当运行顺序为：程序1 → 程序2时，其结果为：X增加2.
当运行顺序为：R1=X; R2=X; R1=R1+1; R2=R2+1; X=R1; X=R2时，其结果为：X增加1.
~~~



<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-07 081637.png" alt="屏幕截图 2023-12-07 081637" style="zoom:33%;" />



这显然是个问题，我们当然不希望一个程序在多次运行之后得到不同的结果。为了解决这个问题，我们可以设置一种互斥关系。例如，如果我们希望上面的例子必须是先运行一个程序再运行另一个程序，我们可以对 X 设置一个互斥访问量。当互斥访问量小于某个特定值时，表示其已经被某个程序占用，暂时不可访问。关于这方面的具体内容，我们到1.3 节再做详细介绍。

​	现在，你已经发现，程序运行起来之后可能体现为和程序不同的性质。静态的程序当然不会出现多个量访问同一个量的情况，只有动态的情况才会出现。为此，当我们要考察程序运行的情况时，就需要在进程的层面做分析。

​	一般地，在 Windows 系统中，我们同时按下 Esc + Shift + Ctrl 就可以调出任务管理器，管理器中的列表体现了计算机中正在运行的进程。那么我们不禁要问，操作系统是如何区分这些进程的呢？如何记录操作系统给进程分配了哪些资源呢？这就引入了 PID 和 PCB。

​	**1.1.3**  **进程控制块（Process Control Block，PCB）** 为了描述和控制进程的运行，系统为每一个进程定义了一个叫进程控制块（PCB）的数据结构。在 PCB 中记录了操作系统所需的、用于描述进程的当前状态及控制进程的**全部**信息。PCB中的信息纷繁复杂，光是代码就得有几百行甚至几千行。其中相对重要的几个数据如下：

~~~
进程描述信息：
1. 进程标识符 (process ID)，唯一，通常是一个整数
2. 进程名，通常基于可执行文件名（不唯一）
3. 用户标识符 (user ID)
4. 家族关系 (process ID)

进程调度信息：
1. 进程状态（就绪/运行/阻塞）
2. 优先级 (priority)
3. 阻塞原因
4. 进程调度所需的其他信息（执行时间总和等）
~~~

​	进程控制块的组织形式有链接方式和索引方式两种。在后续的讨论中，我们一般都认为其采用链接方式组织。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-07 092831.png" alt="屏幕截图 2023-12-07 092831" style="zoom:33%;" />

<div style="page-break-after:always"></div>

## 1.2  进程的控制

​	**1.2.1  进程的基本状态**  正如1.1节中的内容所叙述的，进程具有三种状态：**就绪**、**运行**、**阻塞**。其中就绪状态指的是其等待 CPU 的状态；阻塞状态指的是其等待 I/O 设备的状态。这三种状态具有以下转换图：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-07 084446.png" alt="屏幕截图 2023-12-07 084446" style="zoom: 25%;" />

如果你学习过高中生物的“稳态与调节”部分，你可能会有一种既视感。因为这其实很像细胞外液中血浆、组织液和淋巴间的关系。在这种类比中，“就绪”就是血浆，“运行”就是组织液，“阻塞”就是淋巴。

​	有的操作系统还会把就绪和阻塞各分为两种状态：活动就绪和静止就绪、活动阻塞和静止阻塞。活动态通过**挂起**变成静止态，静止态通过**激活**变成活动态。引起挂起的原因可能是程序运行期间发现了问题、优先级更高的进程需要夺取资源等。这样一来，其转换图就变成了这个样子：

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231207090545972.png" alt="image-20231207090545972" style="zoom: 25%;" />

​	那么操作系统如何控制进程间状态的的转换呢？这就是本节主要要说的进程控制的内容。进程控制主要是靠**原语**。

​	**定义1.2.2（原语）** 原语是指由若干条指令组成、用来实现某个特定操作的一个过程。原语在执行的过程中不能被分割。原语和算法的主要区别就是，原语通常是在硬件层面定义和实现的。进程控制的原语大致有六种：创建原语、撤销原语、阻塞原语、唤醒原语、挂起原语、激活原语。下面我们将分别考察引起原语执行的原因和原语执行的步骤。

​	**1.2.2.1  创建原语**  进程可以继续创建新的进程。进程 $P_i$ 创建了进程 $P_j$ 之后，称 $P_i$ 是 $P_j$ 的父进程，$P_j$ 是 $P_i$ 的子进程。子进程可以继承父进程所拥有的资源。但当子进程撤消时，它要归还从父进程那里获得的资源。撤消父进程时，必须同时撤消其所有子进程。

​	引起创建进程的事件有：用户登录、作业调度、提供服务、应用请求等。

~~~~~~~~
创建原语：
1. 查PCB链表，如果没有空PCB，创建失败，退出原语。
2. 如果有空PCB，取其PCB，填入表项。
3. PCB进入就绪队列，然后入进程表。
4. 返回。
~~~~~~~~

​	**1.2.2.2  撤消原语**  

​	引起撤销进程的事件有：正常结束、异常结束、外界干预等。

~~~
撤销原语：
1. 查进程表，如果没有所要撤销的PCB，撤销失败，退出原语。
2. 如果有这样的PCB，查看其是否有子进程，有则跳到“1”。
3. 如果其没有子进程，释放该进程占有的资源。
4. 释放其占有的PCB。
5. 返回。
~~~

​	**1.2.2.3  阻塞原语、唤醒原语**

​	引起进程阻塞和唤醒的事件有：向系统请求共享资源失败、等待某种操作完成、新数据尚未到达、无新工作可做等。

~~~
阻塞原语：
1. 保存当前进程的PCB现场。
2. 置该进程的状态为阻塞态。
3. 被阻塞进程入等待队列。
4. 转进程调度。
~~~

~~~
唤醒原语：
1. 从等待队列中摘下被唤醒进程。
2. 将该进程置为就绪态。
3. 将被唤醒进程送入就绪队列。
4. 转进程调度或返回。
~~~

​	**1.2.2.4  挂起原语**

​	引起挂起的事件有：用户/父进程的请求、负荷调节的需要、操作系统的需要等。

~~~
挂起原语：
1. 检查当前进程的状态。
2. 如果进程为活动就绪态，将被挂起进程的状态置为静止就绪；
   如果进程为活动阻塞态，将被挂起进程的状态置为静止阻塞；
   如果进程为为运行态，将被挂起进程的状态置为静止就绪，然后转进程调度。
~~~

​	**1.2.2.5  激活原语**

​	引起激活的事件有：有足够的内存空间情况下，用户/父进程的请求。

~~~
激活原语：
1. 检查进程状态是否为静止就绪状态，若否（是静止阻塞），将其置为活动阻塞状态，返回。
2. 若是静止就绪状态，将其置为活动就绪状态。
3. 考察是否可以占用CPU，如果可以占用，转进程调度，否则返回。
~~~

​	***1.2.2.6  Linux系统中的fork函数**

​	为了比较深入地了解父进程和子进程的创建过程，我们在这里简单介绍一下 Linux 系统中用于创建父子进程的fork()函数。

​	如果我们有这样的一个任务：在主程序中，先后 fork() 两个子进程执行客户端 CLIENT(int index) 程序，父进程执行服务端 SERVER 程序。则其代码可能是这样的：

~~~
int main()
{
    pid_t pid1, pid2;
    while ((pid1 = fork()) == -1);
    if (pid1 == 0)
    {
        CLIENT(1);
    }
    else
    {
        while ((pid2 = fork()) == -1);
        if (pid2 == 0)
            CLIENT(2);

        SERVER();
        return 0;
    }
}
~~~

​	通过上述任务，我们可以窥见 fork() 函数的用法：fork() 创建子进程失败，就会返回 -1 ；创建子进程成功，就会返回 0；而如果返回的值既不是 0 也不是 -1，说明返回的是该子进程的父进程。

<div style="page-break-after:always"></div>

## 1.3  进程的同步

​	在学习本节之前，请先自行复习1.1中有关程序运行的**不可再现性**特点。本节的内容比较多，是考研主观题的重要组成部分，请认真学习。

​	**定义1.3.1（进程同步）** 对多个相关进程在执行次序上进行协调，它的目的是使系统中诸进程之间能有效地共享资源和相互合作，从而使程序的执行具有可再现性。

​	**定义1.3.2（临界资源）** 一次只允许一个进程使用的资源。包括软件资源（共享变量等）和硬件资源（打印机等）。

​	每个进程中，访问临界资源的那段代码叫**临界区**。临界区不允许两个以上共享资源的并发进程同时访问的性质称为**互斥**。简而言之，进程的同步就是：在多个进程想要访问临界资源时，通过对进程设定对临界资源的访问权限和访问次序的过程。

​	进程同步应当遵循这样的准则：

~~~
空闲让进：临界资源空闲时，应允许一个请求进入临界区的进程立即进入自己的临界区，以便有效的利用资源。
忙则等待：当临界区正被访问时，其他要求进入临界区的进程必须等待，以保证对临界资源的互斥使用。
有限等待：任何要求访问临界资源的进程应能在有限的时间内进入自己的临界区。
让权等待：不能进入临界区的进程应立即释放CPU。
~~~

这里解释一下**让权等待**的准则：进程因某事件的发生而无法继续运行时，它仍占有 CPU，并通过不断地执行循环测试指令来等待该事件的完成。由此我们其实也可以看出，**让权等待准则并不是一定要满足的**。也就是说，进程无法运行时，并不是一定要让出 CPU 的。

​	**1.3.3  信号量机制**  为了实现进程同步，著名的 $Dijkstra$ 先生引入了**信号量**机制。最初，$Dijkstra$ 把信号量定义为一个有初值的整数，用来表示空闲的资源数。后来，信号量发展成一个结构体，结构体中有两个元素：空闲的资源数和访问该资源的进程等待队列，美其名曰**记录型信号量**。假设对于记录型信号量 $S$ ，其空闲的资源数为 $S.value$，进程等待队列指针为 $S.Next$。如果 $S.value ≥ 0$，则表示有空闲的资源，可以分配；反之， $S.value < 0$ 则表示没有空闲的资源。如果 $S.value$ 的初值为 1，也就是说只允许一个进程访问临界资源，此时的信号量就是互斥信号量。

​	**1.3.3.1  对信号量的操作**

​	对于信号量 $S$，存在两种操作：**$Wait$ 操作**（P操作）和 **$Signal$ 操作** (V操作)。

​	 $Wait$ 操作使得 $S.value - 1$，表示进程请求分配。如果没有空闲的资源，这一进程就要挂到 $S$ 的等待队列上。由上述结论可以推出，当某个信号量的值为负数的时候，这一负数的相反数就是在等待该资源的进程数，也是该记录型信号量等待队列的长度。$Signal$ 操作使得 $S.value + 1$，表示已分配资源的进程要释放资源。这时，只要其值为正，位于等待队列上的队首进程就要获得被释放的资源。其算法实现可能是这样的：

~~~
struct semaphore
{
    int value;
    struct process_control_block *list;
};

void Wait(semaphore S)
{
    S.value--;
    if (S.value < 0)
        block(S.list);
}

void Signal(semaphore S)
{
    S.value++;
    if (S.value >= 0)
        wakeup(S.list);
}
~~~

​	

​	**1.3.3.2  AND型信号量和信号量集**

​	基于以上内容，我们可以做这样的思考：事实上，一个进程的运行可能需要多个空闲的资源。在这样的情况下，我们可以预想，如果一个系统中有 5 个 $\alpha$ 资源，4 个 $\beta$ 资源。进程 A 需要 3 个 $\alpha$ 资源，2 个 $\beta$ 资源，已经获得了 2 个 $\alpha$ 资源，和 2 个 $\beta$ 资源；进程 B 需要 3 个 $\alpha$ 资源，3 个 $\beta$ 资源，已经获得了 3 个 $\alpha$ 资源，和 2 个 $\beta$ 资源。如下表所示：

|          | System | Process A   | Process B   |
| -------- | ------ | ----------- | ----------- |
| $\alpha$ | 5      | 2（Need 3） | 3（Need 3） |
| $\beta$  | 4      | 2（Need 2） | 2（Need 3） |

​	这样一来，由于资源的缺失，这两个进程都无法继续运行，A 在等待 B 释放 $\alpha$ 资源，B 在等待 A 释放 $\beta$ 资源，二者会陷入这样的无限等待之中。这就是**死锁（Deadlock）**。为了避免死锁的出现，我们可以采用**AND型信号量**。其基本思路是：将进程在整个运行过程中需要的所有资源，一次性全部分配给它，待进程使用完后一起释放；只要尚有一个资源无法分配，则其它所有可分配的资源也不分配给它。为此，在 $Wait$ 操作中，增加了一个“与（AND）”条件，加了“与”条件的 $Wait$ 操作叫 $Swait$ (Simultaneous Wait)；类似地，有 $SSignal$ 操作，其定义可能是这样的：

~~~
Swait(src S[], int n)
{
	while (1)
	{
		int i = 1;
		if ( S[1] ≥ 1 && … S[n] ≥ 1)
			for (i = 1; i <= n; i++) Si--;
			break;
		else
			block(S[i].list);	
	}
}

Ssignal(src S[], int n)
{	
	while (1)
	{
		int i = 1;
		for (i = 1; i <= n; i++) 
		{	
			S[i] ++;
			wakeup(S[i].list);
		}
	}
}
~~~

​	另外，前面几种信号量机制中，$Wait(S)$ 和 $Signal(S)$ 操作仅能对信号量 $S$ 进行加 1 或减 1 的操作。而当我们一次需要 N 个某类资源时，并要进行 N 次 $Wait(S)$，非常不方便。对此，我们引入了**信号量集**。其结构体定义可能如下：

~~~
struct SemaphoreSet
{
	int S_value;       // 信号量值
	int inf_value;     // 下限值
	int requirement;   // 需求量
	struct process_control_clock *list;
	//需求队列
};
~~~

​	读者可以自行思考信号量集的 $Swait$ 操作和 $SSignal$ 操作该如何定义。举例来说，$Swait(S,d,d)$ 表示此时在信号量集中只有一个信号量 S，允许它每次申请 d 个资源，当现有资源数少于 d 时不予分配。



​	**1.3.3.3  利用信号量实现互斥访问和前趋关系**

​	这里就是信号量实际应用的层面了。为了使多个进程可以互斥地访问某临界资源，只需要为这一资源设置一个初值为 1 的互斥信号量 $mutex$，然后将各进程访问该资源的临界区置于该信号量的 $Wait(mutex)$ 操作和 $Signal(mutex)$ 操作之间即可，即：

~~~
semaphore mutex = 1;
wait(mutex);
临界区;
signal(mutex);
~~~

​	注意，两个操作必须**成对出现**。对于一个完整的操作过程中（不一定是一个进程），不能只有 $Wait$ 而没有 $Signal$，也不能只有 $Signal$ 而没有 $Wait$ 。

​	另外，信号量还可以实现前趋关系。设有两个进程 $P_1、P_2$，$P_1$ 中有语句 $S_1$，$P_2$ 中有语句 $S_2$，要求二者的运行顺序为 $S_1 \rightarrow S_2$，这样一来：我们可以让$P_1、P_2$ 共享一个信号量 $S$，设其初值为 0，然后这样写：

~~~
P_1:
	S_1; Signal(S);
	
P_2:
	S_2; Wait(S);
~~~

即，将 $Signal(S)$ 操作放在语句 $S_1$ 后面，而在 $S_2$ 语句前面插入 $Wait(S)$ 操作。一般地，对于具有多个前趋的语句，其前趋有 $k$ 个，我们就需要在这一语句的前面对 $k$ 个信号量做 $Wait$ 操作（或者使用信号量集）；对于具有多个后继的语句，其后继有 $k$ 个，我们就需要在这一语句的后面对 $k$ 个信号量做 $Signal$ 操作（或者使用信号量集）。

​	**1.3.3.4  经典的同步-互斥问题**

​	*（一）生产者-消费者问题*

​	我们可以把系统中使用某一类资源的进程称为消费者，而把释放同类资源的进程称为该资源的生产者。对于一个长度为 $n$ 的有界缓冲区，生产者会向里面释放资源，消费者会从里面取出资源，每个资源占 1 个缓冲区大小。在这个过程中，对于每个缓冲区，生产者和消费者自然是要互斥访问的；只有当缓冲区内存在空位时，生产者才能去释放；只有当缓冲区内有资源时，消费者才能去取。也就是说，我们需要 $n + 2$ 个信号量。

​	下面的伪代码是上述问题在缓冲区大小为 10 时的实现，使用了记录型信号量。在阅读下面的代码之前，请读者先自行思考记录型信号量实现方案。

~~~
const int N = 10;
product BUFFER[N];
semaphone s[N] = {1};
semaphone empty = 10, full = 0;
int in = 0, out = 0;

void Producer()
{
	while(1)
	{
		Wait(emp);
		Wait(S[in]);
        生产产品 x;
        BUFFER[in] = x;
        Signal(S[in]);
        in = (in + 1) % 10;   
        Signal(full);
        
    }
}

void Consumer()
{
	while(1)
	{
		Wait(full);
		wait(S[out]);
		消费产品;
		BUFFER[out] = NULL;
		Signal(S[out]);
		out = (out + 1) % 10;
		Signal(empty);
	}
}
~~~

​	同样地，这一实现还可以使用AND型信号量来实现。读者可以自行给出实现过程。

​	

​	*（二）读者-写者问题*

​	一个数据文件或记录，可被多个进程共享，我们把只要求读该文件的进程成为“读者进程”，其他进程则称为“写者进程”。允许多个进程同时读一个共享对象，因为读操作不会使数据文件混乱。但不允许一个写者进程和其他读者进程或写者进程同时访问共享对象。因为这种访问将会引起混乱。也就是说，读操作和读操作不互斥，读操作和写操作互斥，写操作和写操作互斥。当读操作和写操作互斥时，我们让**读者优先**，即：等所有声明要读的读者都读完后，第一位写者才能进去写。这里的临界资源有两个：一是文件写的权限，我们把控制它的信号量记为 $wmutex$；二是读者的数量 $readcount$，我们把控制它的信号量记为 $rmutex$。

​	下面的伪代码是上述问题的实现，使用了记录型信号量。在阅读下面的代码之前，请读者先自行思考记录型信号量实现方案。

~~~
semaphore rmutex = 1, wmutex = 1;
int readcount = 0;
void Reader()
{
	do
	{
		Wait(rmutex);
		if(readcount == 0) Wait(wmutex);
         readcount ++;
		Signal(rmutex);
		
         读操作;
         
         Wait(rmutex);
         readcount --;
         if (readcount == 0) Signal(wmutex);
		Signal(rmutex);
	}while(1);
}

void Writer()
{
	do
	{
		Wait(wmutex);
		写操作;
		Signal(wmutex);
	}while(1);
}
~~~

​	通过上述代码我们可以看到，如果申请访问文件的队列为：
$$
读者1 \rightarrow 写者1 \rightarrow 读者2 \rightarrow 写者2
$$
​	那么整体的流程大致是这样的（假设两位读者进入之后暂时都不出来）：

~~~
读者1请求，进入。readcount = 1，rmutex = 1，wmutex = 0.
写者1请求，被挂在wmutex的等待队列上。readcount = 1，rmutex = 1，wmutex = -1.
读者2请求，进入。readcount = 2，rmutex = 1，wmutex = -1.
写者2请求，被挂载wmutex的等待队列上。readcount = 1，rmutex = 1，wmutex = -2.
~~~

​	这一问题还有很多的变式。比如写者优先的情况（读-写互斥，写-写互斥，有写请求则不能读）和只允许 $R_N$ 个读者同时读的情况。对于**前者**，我们需要引入新的临界资源 $writecount$、控制其访问的信号量 $wcmutex = 1$、控制有写请求则不能读的信号量 $S = 0$。对于原来的 $rmutex$ 和 $wmutex$，其内涵并无更新。其大致实现如下。在这里，我使用 $P$ 表示 $Wait$，$V$ 表示 $Signal$。

~~~
semaphore rmutex = 1, wmutex = 1, wcmutex = 1, S = 1;
int readcount = 0, writecount = 0;
void Reader()
{
	do
	{
		P(S);
		P(rmutex);
		if(readcount == 0) P(wmutex);
         readcount ++;
		V(rmutex);
		V(S);
		
         读操作;
         
         P(rmutex);
         readcount --;
         if (readcount == 0) V(wmutex);
		V(rmutex);
	}while(1);
}

void Writer()
{
	do
	{
		P(wcmutex);
		if(writecount == 0) P(S);
		writecount ++;
		V(wcmutex);
		P(wmutex);
		
		写操作;
		
		P(wcmutex);
		writecount --;
		if(writecount == 0) V(S);
		V(wcmutex);
		V(wmutex);
	}while(1);
}
~~~

​	对于后者，我们可以使用信号量集等方式来实现。具体的实现留给读者自己思考。



​	*（三）哲学家进餐问题*

​	设有五个哲学家共用一张圆桌，分别坐在周围的五张椅子上，在圆桌上有五个碗和五只筷子，他们的生活方式是交替地进行思考和进餐。平时，一个哲学家进行思考，饥饿时便试图取用其左右最靠近他的筷子，只有他拿到两只筷子时才能进餐。进餐完毕，放下筷子继续 思考。整个桌子的情况如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-12 101906.png" alt="屏幕截图 2023-12-12 101906" style="zoom:33%;" />

​	对于这个问题，我们很容易想到，可以使用一个信号量表示一只筷子。这样一来，五个信号量构成信号量数组 $chopsticks[5] = \{1，1，1，1，1\}$，这样一来，第 $i$ 位哲学家的操作可以理解为：

~~~
do
{
	P(chopsticks[i]);
	P(chopsticks[(i+1) % 5]);

	吃操作；

	V(chopsticks[i]);
	V(chopsticks[(i+1) % 5]);

	思考操作；
	
}while(1);

~~~

​	然而，上述的描述中没有考虑**死锁**的问题。在上面操作的定义中，每位哲学家都先拿起自己左边的筷子，然后再拿右边的筷子。那么如果每位哲学家都拿起了自己左边的筷子，他们就都在等待右边的筷子被释放，而同时又拿着左手边哲学家的右筷子不放，这样一来每个哲学家到死都吃不上饭。这就是死锁状态。

​	其实我们很容易想到上述问题的解决方案，例如，仅当哲学家的左、右两只筷子均可用时，才允许他拿起筷子进餐。这就可以使用**AND型信号量**来实现同步。另外，还可能有这样的解决思路：至多只允许有四位哲学家同时去拿左边的筷子，最终能保证至少有一位哲学家能够进餐，并在用毕时能释放出他用过的两只筷子，从而使更多的哲学家能够进餐。这就可以使用简单的信号量实现同步。我们请读者使用 $P$ 和 $V$ 两种操作自行写出这一问题的伪代码。

<div style="page-break-after:always"></div>

## 1.4  进程间通信

​	当进程运行起来之后，每个进程只能访问给自己分配的资源，不能访问其他进程的资源。但是，在进程之间是可以传递消息的。这就是进程间的通信。

​	**定义1.4.1（进程通信）** 进程之间实现资源共享或信息交换的过程。一般地，进程通信有两种情况。一种是直接通信，即发送进程利用操作系统所提供的发送命令，直接把消息发送给接收进程；另一种是间接通信，即建立一个信箱，发送进程把消息发到信箱之中，接收进程从信箱中取消息。

​	**1.4.2  消息队列** 消息队列通信机制通过内存中公用的消息缓冲区进行进程通信。发送进程发送消息时，需申请一个消息缓冲区，把自己的进程标识符和消息内容填入消息缓冲区，然后将该消息缓冲区插入到接受进程的消息缓冲队列中；接收进程接收消息时，需从自己的消息缓冲队列中摘下一个消息缓冲区，取出其中的消息，然后把消息缓冲区归还给系统。从直接与间接的角度看，这是妥妥的间接通信。我们常说，计算机学科就是数据结构加算法的学科。对于这种机制，我们自然要学习其中的数据结构和算法。只不过，在这里的“算法”，由于是硬件层面实现的，所以还是应该叫“原语”。

​	在消息队列通信机制中，存在如下数据结构：

~~~
消息缓冲区：
struct message_buffer
{
	int sender;    // 发送者进程标识符
	int size;      // 消息长度
	char *text;    // 消息正文
	struct message_buffer next; 
	// 指向下一个消息缓冲区的指针
};

PCB 中有关通信的数据项：
struct message_buffer mq; // 消息队列队首指针
semaphore mutex;          // 消息队列互斥信号量
semaphore sm;             // 消息队列资源信号量
~~~

这些数据结构的存在性是很容易理解的，我们在非实践性的学习中可以不重点记忆。

​	接下来我们来看看原语。我们很容易理解，涉及到消息传递的原语无非就那么两种：发送（Send）和接收（Receive）。**发送原语**的过程大致如下图所示：

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231212111657553.png" alt="image-20231212111657553" style="zoom:33%;" />

发送进程在利用发送原语发送消息之前，应先在自己的内存空间，设置一发送区 $a$ ，把待发送的消息正文、发送进程标识符、消息长度等信息填入其中，然后调用发送原语，把消息发送给目标接收进程。发送原语首先根据发送区 $a$ 中所设置的消息长度 $a.size$ 来申请一缓冲区 $i$ ，接着，把发送区 $a$ 中的信息复制到缓冲区 $i$ 中，然后将 $i$ 挂在接收进程的消息队列 $mq$ 上。至于**接收原语**，那就很简单了。无非就是把缓冲区中发给自己的信息取出来处理。

​	我们来看一个 $Linux$ 操作系统下的消息队列实例：

~~~
#include <bits/stdc++.h>
#include <sys/ipc.h>
#include <sys/msg.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>
#define KEY 75
using namespace std;

int msgqid;

struct msgform
{
    long mtype; // 1，表示C1 to Server
                // 2. 表示C2 to Server
                // 3，表示Server to C1
                // 4，表示Server to C2
                // 设定为long，是因为msgrcv的对应位置mtype为long

    int index;        // 表示第index个消息
    char mtext[1024]; // 消息的文本长度
};

void CLIENT(long num)
{
    msgform msg, msg_rcv;
    int i = 10;
    msgqid = msgget(KEY, 0777); // 打开消息队列
    while (i)
    {
        msg.mtype = num;
        msg.index = i;
        sprintf(msg.mtext, "(Server) received from CLIENT %ld, No.%d\n", num, msg.index);
        msgsnd(msgqid, &msg, 1024, 0);
        printf("(client %ld) sent msg %d\n", num, msg.index);
        i--;
        sleep(1);

        msgrcv(msgqid, &msg_rcv, 1024, num + 2, 0);
    }
    exit(0);
}

void SERVER()
{
    msgform msg_s1, msg_s2;
    int i = 20, j = 10;
    msgqid = msgget(KEY, 0777 | IPC_CREAT);
    while (i)
    {
        msgrcv(msgqid, &msg_s1, 1024, -2, 0);
        printf("%s", msg_s1.mtext);
        msg_s1.mtype += 2;
        sprintf(msg_s1.mtext, "(Client %ld) received from SERVER, No.%d\n", msg_s1.mtype - 1, msg_s1.index);
        msgsnd(msgqid, &msg_s1, 1024, 0);
        
        sleep(1);
        i--;
        printf("Server has sent to Client %ld", msg_s1.mtype - 2);
    }
    msgctl(msgqid, IPC_RMID, 0); // 消除消息队列的标识符
    exit(0);                     // 退出
}

int main()
{
    pid_t pid1, pid2;
    while ((pid1 = fork()) == -1);
    if (pid1 == 0)
    {
        CLIENT(1);
    }
    else
    {
        while ((pid2 = fork()) == -1);
        if (pid2 == 0)
            CLIENT(2);

        SERVER();
        return 0;
    }
}
~~~

​	这个实例可能相对复杂，我们来稍微解释一下。它实现了下面的内容：

~~~
用一个程序作为“引子”，先后fork()两个子进程，父进程执行服务端SERVER程序，两个子进程分别执行客户端CLIENT 1程序和客户端CLIENT 2程序，进行通信。
由SERVER端创建一个Key为75的消息队列，等待CLIENT 1端进程和CLIENT 2端进程发来的消息。当收到CLIENT 1端消息编号为1的消息，以及收到CLIENT 2端消息编号为1的消息后，作为结束信号，删除该队列，并结束SERVER。
其中，SERVER每接受到一个消息后显示一句“(SERVER) received message [编号] from CLIENT [编号]”，然后发送一个返回消息给对应CLIENT端，显示一句“(SERVER) sent return message [编号] of CLIENT [编号]”。
CLIENT 1端使用key为75的消息队列，先后发送消息编号从10到1的消息，然后退出。CLIENT 1每发送一条消息后显示一句“(CLIENT 1) sent message [编号]”,然后等待接受SERVER端返回给自己的消息后，显示一句“(CLIENT 1) received return message [编号] from SERVER”，再发送下一条消息。
CLIENT 2端使用key为75的消息队列，先后发送消息编号从10到1的消息，然后退出。CLIENT 2每发送一条消息后显示一句“(CLIENT 2) sent message [编号]”,然后等待接受SERVER端返回给自己的消息后，显示一句“(CLIENT 2) received return message [编号] from SERVER”，再发送下一条消息。
~~~

​	首先，`msgsnd`、`msgrcv` 和 `msgctl` 是 $Linux$ 操作系统中用于操作消息队列的三个关键函数。

​	`msgsnd`用于将消息发送到消息队列中。它接收消息队列标识符 `msqid`，一个指向消息结构体的指针 `msgp`，消息的大小 `msgsz` 以及消息发送的标志 `msgflg`。消息队列的长度是有限的，如果消息队列满了，那么发送消息的进程可能会被阻塞。
​	`msgrcv` 用于从消息队列中接收消息。它接收消息队列标识符 `msqid`，一个指向消息结构体的指针 `msgp`，消息的大小 `msgsz`，要接收的消息类型 `msgtyp` 以及消息接收的标志 `msgflg`。如果没有与指定类型匹配的消息，接收进程可能会被阻塞。当消息类型`msgtyp`为负数 $-k(k > 0)$ 时，表示接收消息的类型值小于等于 $k$.
​	`msgctl` 用于对消息队列进行控制操作，例如删除消息队列。`cmd` 参数指定要执行的操作，常见的操作包括 $IPC\_RMID$（删除消息队列）、$IPC\_SET$（设置消息队列的属性）等。

​	在代码中，我们定义了一个消息队列的键值 $KEY$，当CLIENT端和SERVER端想要连接消息队列的时候就要使用这个值。另外，我们定义了`msgform`，用于描述消息的内容。其中`mtype`是最重要的值。重要的原因在于：应当保证任何一端不要拿到自己发出去的消息，也不要拿到发给别人的消息。对此，我们使用 4 种值完成了实现。在这种情况下，CLIENT 1 就要接收类型为 3 的消息，CLIENT 2 就要接收类型为 4 的消息，SERVER 就要接收类型为 1 和 2 的消息。

​	有了上述补充内容，相信你可以通过自行探索读懂上面的程序。如果你能够完全读懂，就代表你已经基本掌握进程间通信这一块内容了。

<div style="page-break-after:always"></div>

## 1.5  线程

​	通过上述有关进程的内容，我们看到，进程同时是资源分配单位和 CPU 调度单位，而进程所耗费的时空开销则是同时由资源分配和 CPU 调度影响的。为此，我们考虑将上述两个属性分开，这就引入了线程。

​	**定义1.5.1（线程）** CPU 的基本调度单位。有了线程之后，进程只作为最基本的资源分配单位。线程的运行也有就绪、运行、阻塞三种状态。可以认为，线程是进程的子集，一个进程的所有线程构成了这个进程的一个划分。

​	进程和线程可以从调度性、并发性、拥有资源的情况、独立性进行比较。

​	从调度性上来看，传统的操作系统中资源分配调度的单位都是进程。而在引入线程的操作系统中，则把线程作为调度和分派的基本单位，而把进程作为资源拥有的基本单位。
​	从并发性上来看，进程之间的运行有并发性，线程之间的运行也有并发性。
​	从拥有资源情况来看，拥有资源的基本单位都是进程。线程除了一点在运行中必不可少的资源外，本身基本不拥有系统资源，但它可访问其隶属进程的资源。
​	从独立性看，进程之间的独立性较高，同一进程的不同线程的独立性较低，不同进程的不同线程的独立性较高。

​	**1.5.2  线程控制块（TCB）** 如同每个进程有一个进程控制块一样，系统也为每个线程配置了一个线程控制块 TCB ，将所有用于控制和管理线程的信息记录在线程控制块中。其中的主要内容包括：

~~~
线程标识符;
一组寄存器。包括程序计数器、状态寄存器、通用寄存器;
线程状态;
优先级;
线程专用存储器;
堆栈指针.
~~~

<div style="page-break-after:always"></div>

## 习题

1. 假定有一个信箱可存放 $N$ 封信。当信箱不满时发信者可把信件送入信箱；当信箱中有信时收信者可从信箱中取信。用指针 $R、K$ 分别表示可存信和取信的位置，请用记录型信号量来管理这个信箱，使发信者和收信者能正确工作。
2. 哲学家进餐问题有多种解决死锁的方法，试用记录型信号量写出下面这种解决方法的算法：

~~~
规定奇数号哲学家先拿他左边的筷子，然后再去拿右边的筷子；而偶数号哲学家则相反。按此规定，将是 1 、 2 号哲学家竞争 1 号筷子； 3 、 4 号哲学家竞争 3 号筷子。即五位哲学家都先竞争奇数号筷子，获得后，再去竞争偶数号筷子，最后总会有一位哲学家能获得两只筷子而进餐。
~~~

<div style="page-break-after:always"></div>

# 第二章  处理机调度、死锁

## 2.1   处理机调度绪论

​	本节的内容比较“文”，也比较枯燥，但是经常出现在考研操作系统的前几个选择题中，所以还是不得不写一写的。读者可以**不求甚解**，重在能让你“欣然忘食”的“会意”。

​	**2.2.1  处理机的调度** 调度，就是资源分配的过程。处理机的调度，就是 $CPU$ 中完成资源分配的过程。

​	在上述定义的基础上，我们可以设想一个极其简单的过程：当我们把一个作业丢到计算机中，操作系统使这个作业单独进入内存并独占系统资源，直到其运行结束才允许下一个作业进入内存。这样的操作系统就是**单道批处理（操作）系统**。其中，**批处理**的含义是，用户将一批作业提交给操作系统后就不再干预，由操作系统控制它们运行。单道批处理系统具有很明显的缺点，即：当作业进行 I/O 操作时，CPU 只能处于等待状态，因此，CPU 的利用率较低。对此，我们当然可以采用流水线的思想：当某个作业在等待 I/O 时，CPU 调度另一个作业运行。这样的操作系统就是**多道批处理（操作）系统**。

​	“单道”和“多道”，是以 CPU 在**空间上**调度作业的方式来分类的。如果我们以 CPU 在**时间上**调度作业的方式来分类，则可以把操作系统分为**分时系统**和**实时系统**。

​	**分时系统**的特点是，计算机把时间分为许许多多的**时间片**，操作系统把时间与内存空间按一定的时间间隔，轮流地切换给各作业使用。由于时间间隔很短，每个作业的“感觉”就像自己独占计算机一样。类似地，这种思想也可以用在多用户计算机中。**实时系统**的“实时”，是表示“及时”。指系统能及时响应外部事件的请求，在规定的时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。实时系统中的任务有两种：强实时（HRT）任务和软实时（SRT）任务。HRT 任务要求强有力地保证任务处理的时间，SRT 任务要求基本保证任务处理的时间。

​	那么，CPU 该如何调度作业、又应该调度哪个作业呢？这就是我们这一章需要讨论的问题。容易理解的是，一个作业从提交到获得处理机执行，直至作业运行完毕，可能需要经历多级处理机调度，即多级资源分配。

​	**2.1.2 处理机调度的层次** 处理机调度有三大层次：高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）。

​	**高级调度（作业调度）**用于决定把外存上处于后备队列中的哪些作业调入内存，为它们分配必要的资源，并创建进程。容易理解的是，在批处理中，大多配有作业调度。作业调度的运行频率较低，一般为几分钟一次。
​	**中级调度（内存调度）**按一定的算法将外存中处于静止就绪状态或静止阻塞状态的进程换入内存，而将内存中处于活动就绪状态或活动阻塞状态的某些进程换出至外存。
​	**低级调度（进程调度）**的主要任务是按照一定的策略选取就绪队列中的一个进程获得处理机 。进程调度是最基本的调度，在操作系统中中**必须**配置它 。它的运行频率很高，典型的情况是几十毫秒便进行一次，因此它的调度算法不能太复杂以免占用太多的 CPU 时间 。进程调度可采用下述两种方式：非抢占方式和抢占方式。

~~~
非抢占方式是指：一旦进程获得 CPU，它将一直执行。直至该进程完成或发生事件而阻塞时，才将 CPU 分配给其他进程。
抢占方式是指：当一进程正在处理机上执行时，系统可根据某种原则暂停它的执行，并将已分配给它的处理机重新分配给另一个进程。
~~~

​	**2.1.3  处理机调度的目标** 处理机调度算法，就是为了解决怎样调度作业、调度哪个作业的问题而提出来的。处理机调度算法的共同目标是：提高系统的资源利用率、尽量使每个进程都获得合理的 CPU 时间、保持系统资源使用的平衡性。其中，系统资源利用率的衡量标准就是 CPU 的利用率：
$$
CPU 的利用率 = \frac{CPU有效工作时间}{CPU有效工作时间 + CPU空闲等待时间}
$$
进程获得合理的 CPU 时间是指，不要发生**饥饿**现象，即：不要出现有作业一直没有被调度的情况。

​	对于上面提到的三种操作系统（批处理、分时、实时），它们在实现的过程中各有一些特殊的目标。

​	**批处理系统**追求平均周转时间短、系统吞吐量高、CPU 利用率高。其中，平均周转时间的定义为：
$$
T = \frac{1}{n}\sum_{i = 1}^n T_i
$$
实际上，我们还可以定义平均带权周转时间。其中带权周转时间定义为作业的周转时间 $T$ 与系统为它提供服务的时间 $T_s$ 之比，所以平均带权周转时间的公式为：
$$
W = \frac{1}{n}\sum_{i = 1}^n \frac{T_i}{T_s}
$$
吞吐量指的是：单位时间内系统所完成的作业数。容易理解的是，作业长度越短，吞吐量就越大。我们容易想象，如果使用贪心策略盲目地先运行短长度的作业从而追求吞吐量高，就很有可能导致饥饿现象。
	**分时系统**追求响应时间快和均衡性。响应时间就是从作业提交到作业开始运行的时间；均衡性指的是系统响应时间的快慢应与用户所请求服务的复杂性相适应，任务越短，允许的响应时间应当越短。
	**实时系统**追求截止时间的保证和请求的可预测性。截止时间是指任务最迟执行时间或最迟完成时间，对此的追求自然有 HRT 任务和 SRT 任务之分；可预测性是指通过已有的任务预测下一个任务，如果预测准确，则可以提高实时性。

<div style="page-break-after:always"></div>

## 2.2  作业调度

​	**定义2.2.1（作业）** 在多道批处理系统中，作业（Job）是用户提交给系统的一项相对独立的工作。包括通常的程序与数据，还配有作业说明书，系统根据该说明书控制程序运行。

​	**定义2.2.2（作业步）** 作业运行时，经过若干相对独立又相互关联的顺序加工步骤后得到结果，其中的加工步骤称为作业步（Job Step）。作业步间存在联系，一般是上个作业步的输出作为下个作业步的输入。一个典型的作业可分成编译作业步，链接装配作业步，运行作业步。

​	**2.2.3  作业控制块（Job Control Block，JCB）** 为了管理和调度作业，在多道批处理系统中为每个作业设置了一个作业控制块，它是作业在系统中存在的标志，其中保存了系统对作业进行管理和调度所需的全部信息。JCB 中包含的内容一般有作业 ID、用户信息、作业类型（I/O 繁忙型、CPU繁忙型、批量型等）、作业状态、调度信息、资源要求等。说到**状态**，作业从进入系统到运行结束，通常需要经历收容、运行和完成三个阶段。相应的作业也就有“后备状态”、“运行状态”和 “完成状态”。

​	**2.2.4  作业调度的算法**  作业调度的算法，就是决定把多少个作业调入内存、把哪些作业调入内存的过程。这样的算法有三种：先来先服务（FCFS）算法、短作业优先（SJF）算法、优先级调度算法。

​	**先来先服务（FCFS）算法**  这是最简单、最容易理解的调度算法，可用于各种调度。在这个算法中，系统将按照作业到达的先后次序来进行调度。或者说，它是优先考虑在系统中**等待时间最长**的作业。从后备作业队列中选择几个最先进入该队列的作业，将它们调入内存。
​	**短作业优先（SJF）算法**  这是一种贪心算法。通过 2.1 中的陈述，你会发现，这是一个盲目地追求吞吐量高的算法。该算法的缺点是：必须预知作业的运行时间、长作业可能出现饥饿现象、不能保证紧迫性作业能得到及时处理。
​	**优先级调度算法**  根据作业的紧迫程度赋予优先级，调度算法根据优先级进行调度。在多种多样的优先级调度算法中，有一种**高响应比优先调度（HRRN）算法**最为好用。在批处理系统中，FCFS 算法所考虑的只是作业的等待时间，而忽视了作业的运行时间。而 SJF 算法正好与之相反，只考虑作业的运行时间，而忽视了作业的等待时间。HRRN 算法既考虑作业的等待时间，又考虑作业运行时间，也就是说，既照顾了短作业，又不致使长作业的等待时间过长，从而改善了处理机调度的性能。为此，我们给每个作业引入一个动态优先级 $P$，令它随等待时间延长而增加，该优先级的变化规律可描述为：
$$
P = \frac{T_{wait} + T_{ask}}{T_{ask}}
$$
其中 $T_{wait}$ 表示这一作业当前已经等待的时间长度，$T_{ask}$ 表示这一作业要求的服务时间长度。由于等待时间与服务时间之和就是系统对该作业的响应时间 $T_{sys}$，故该优先级又相当于**响应比 $R_p$**：
$$
R_p = \frac{T_{sys}}{T_{ask}}
$$
因此这一算法才叫“高响应比优先调度算法”。

<div style="page-break-after:always"></div>

##  2.3  进程调度

​	在学习本节之前，我们先整合一下前面的课程有关进程调度的叙述。从 1.2 节的内容我们可以得知：进程可以通过进程调度从就绪态变成运行态。从 2.1 节的内容我们可以得知：进程调度是操作系统中最底层的调度，也是最基本的调度。只要是操作系统都要有进程调度；进程调度有两种方式：抢占方式和非抢占方式。

​	在本节对于进程调度的讨论中，我们将分为分时系统和实时系统两个部分。在分时系统中，进程调度的任务主要有：保存 CPU 的现场信息、按某种算法选取进程、把 CPU 分配给进程；在实时系统中，进程调度的任务主要有：保证进程运行的截止时间、按某种算法选取进程、把 CPU 分配给进程。

​	**2.3.1  分时系统中的进程调度算法** 首先，我们敬告读者：**学知识不要学得太死**。2.2 节中讲到的有关作业调度的 FCFS 算法和优先级调度算法都可以用于进程调度。请同学们参考。

​	**时间片轮转调度（Round Robin，RR）算法** 这是分时系统中基本的进程调度算法之一。进程的切换时机分为两种情况：
~~~
若一个时间片尚未用完，正在运行的进程便已经完成，就立即激活调度程序，将它从就绪队列中删除、再调度就绪队列中队首的进程运行，并启动一个新的时间片。
若一个时间片用完，进程尚未运行完成，计时器中断处理程序被激活，调度程序将把它送往就绪队列的末尾。
~~~

显然，在这一算法中，时间片的长度是很重要的因素。如果时间片太长，所有进程都会在一个时间片之内运行完，那就退化成了 FCFS 算法；如果时间片太短，用户的一次请求需要多个时间片才能处理完，上下文切换次数增加，系统的开销也会增加。较为可取的时间片大小是略大于一次典型的交互所需要的时间。
	**多级反馈队列调度算法** 在如前所述的调度算法中，系统中仅设置一个进程的就绪队列，可能无法满足不同进程优先级的要求。为此，我们可以设置多个队列，在队列之间设置优先级，在队列内使用 FCFS 算法，把刚从 CPU 上下来的进程挂到最低优先级队列的末尾。如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-14 090045.png" alt="屏幕截图 2023-12-14 090045" style="zoom: 25%;" />

在多级反馈队列调度算法中，如果规定第一个队列的时间片略大于多数人机交互所需之处理时间时，便能较好地满足各种用户的需求。
	**保证调度算法** 本算法所“保证”的是进程调度的公平性。如果在系统中有 $n$ 个相同类型的进程同时运行，为公平起见，该算法保证每个进程都获得相同的处理机时间 $\frac{1}{n}$。该算法的步骤大致如下：

~~~
设置每个进程的最低获得处理机时间比率，例如进程 A 的最低比率为0.5，而进程 B 的最低比率为0.8，进程 C 的最低比率为1.2等；
跟踪计算每个进程自创建以来已经执行的处理时间；
计算每个进程应获得的处理机时间，即自创建以来的时间除以 n；
计算进程获得处理机时间的比率，即进程实际执行的处理时间和应获得的处理机时间之比；
比较各进程获得处理机时间的比率，调度程序选择比率最小的进程分配 CPU，并让该进程一直运行，直到超过其最低比率为止。
~~~

​	**公平共享调度算法** 本算法追求多用户间的“公平”共享。我们来举一个简单的例子：系统中有两个用户，用户 1 拥有 $4$ 个进程 $A,B,C,D$，用户 2 只有一个进程 $E$ 。为保证两个用户能获得相同的处理机时间，则必须执行如下所示的强制调度序列：
$$
A,E,B,E,C,E,D,E,A,E,B,E,C,E,D,E,...
$$
如果希望用户 1 所获得的处理机时间是用户 2 的两倍，则必须执行如下所示的强制序列：
$$
A,B,E,C,D,E,A,B,E,C,D,E,...
$$
这样一来，就可以保证多用户间进程运行的公平性。

​	**2.3.2  实时调度算法** 实时系统中的调度也叫**实时调度**。实时调度要求快速的中断响应能力和任务分派能力。

​	这里我们要再次敬告读者：学知识不要学得太死。有的读者看了前面的内容之后，可能会有这样的想法：分时系统和实时系统的概念就是完全分开的，毫不重合的。事实上，实时系统中也可以分时间片。`就像市场不是资本主义社会的专利，社会主义社会中也有市场`。实时系统所要求的截止时间可以分为两种：**开始截止时间**和**完成截止时间**。开始截止时间是指任务开始运行的 $deadline$，完成截止时间是指任务完成运行的 $deadline$。当我们引入时间片的概念之后，这两种截止时间就可以体现在多个时间片上。例如，我可以要求任务 $A$ 在时间片 $1,5,9$ 开始之前必须至少运行过一次，任务 $B$ 在时间片 $2,6,10$ 结束之前必须至少运行过一次。

​	接下来，我们将介绍一个非抢占式的算法和两个抢占式的算法。

​	**非抢占式最早截止时间优先（Earliest Deadline First，EDF）算法** 这是一种贪心算法，根据任务的**开始截止时间**来确定任务的优先级。开始截止时间越早，调度得越早。
​	**抢占式 EDF 算法** 这也是一种贪心算法，根据任务的**完成截止时间**来确定任务的优先级。下图是这一算法的一个解释：

<img src="C:\Users\21117\Pictures\Typora\009c7db82fb54ceb8bccb06b5d16f75a.png" alt="009c7db82fb54ceb8bccb06b5d16f75a" style="zoom: 50%;" />

​	**最低松弛度优先（Least Laxity First，LLF）算法** 这是一种抢占式的调度算法。最低松弛度，就是最高紧急度。也就是说，本算法是根据任务紧急的程度来确定任务的优先级。紧急程度越高，优先级当然就越高。其中松弛度 $\rho$ 的定义如下：
$$
\rho = T_{must} - T_{run} - t
$$
其中，$T_{must}$ 表示任务必须完成的时间，$T_{run}$ 表示任务本身要运行的时间，$t$ 表示当前时间。实际上，这和抢占式 EDF 算法的思路是不谋而合的。我们通过下面的例子来描述一下这一算法：有两个周期性实时任务 A 和 B，任务 A 要求每 $20ms$ 执行一次，执行时间为 $10ms$；任务 B 要求每 $50ms$ 执行一次，执行时间为 $25ms$，则二者的运行情况大致如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-14 101732.png" alt="屏幕截图 2023-12-14 101732" style="zoom:33%;" />

​	**定义2.3.3（优先级倒置）** 高优先级进程被低优先级进程延迟或阻塞的现象。这一现象的发生一般具有这样的条件：存在三个及以上进程、存在临界资源。
​	举例来说，有三个完全独立的进程，优先级由高到低排列为 $P_1,P_2,P_3$。$P_1$ 和 $P_3$ 通过一个共享的临界资源 $CS-3$ 进行交互。代码如下：

~~~
P1: P(Mutex); ulitize(CS-3); V(Mutex);
P2: ... program2 ...
P3: P(Mutex); utilize(CS-3); V(Mutex);
~~~

假如 $P_3$ 最先执行，在执行了 $P$ 操作后，进入到临界区，接下来发生了这样的一系列事情：
~~~
时刻a： P_2 就绪，抢占了 P_3 的处理机而运行;
时刻b： P_1 就绪，抢占了 P_2 的 处理机而运行;
时刻c： P_1 执行 P 操作，阻塞 P_2 继续运行;
时刻d： P_2 运行完毕, P_3 接着运行;
时刻e： P_3 退出临界区，唤醒 P_1；因 P_1 优先级高，抢占 P_3 的处理机而运行。
~~~

则如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-14 102957.png" alt="屏幕截图 2023-12-14 102957" style="zoom: 50%;" />

上述情况违背了优先级高的进程优先执行的原则，不应该出现在任何系统中。

​	优先级倒置问题的解决思路无非就是：不允许进入临界区后的进程所占有的 CPU 资源被抢占。也就是说，只要该进程访问了临界资源，无论它原来的优先级有多低，都要把它的优先级提到最高。具体的解决方案有两种，一是预防性的**优先级天花板**，二是解决性的**动态优先级继承**。说起来很高大上，实际上是一码事。

​	**优先级天花板**方案就是把正在访问某临界资源的进程的优先级提升到要访问该资源的所有进程的优先级中最高的那个，即提升到访问该资源的所有进程的优先级的“天花板”。这样一来，就可以**预防**优先级倒置现象的出现。**动态优先级继承**方案则是：在发现高优先级任务被低优先级任务资源阻塞之后，再提升低优先级任务的优先级。

<div style="page-break-after:always"></div>

## 2.4  死锁与银行家算法

​	**定义2.4.1（死锁）** 死锁（$Deadlock$）是指多个并发进程彼此等待对方所拥有的资源，且这些并发进程在得到对方的资源之前不会释放自己所拥有的资源，从而使得各进程不能继续向前推进。死锁的形成有两个基本原因：资源不足、进程推进顺序不当。

​	**2.4.2  死锁的必要条件** 死锁的产生有四大**必要条件**：互斥条件、不可抢占条件、请求和保持条件、环路等待条件。要**预防**死锁，我们只需要破坏这四大条件中的任何一条即可。

​	**互斥条件**是指：并发进程所要求和占有的资源是不能同时被两个以上进程访问。这个当然是资源本身的性质，我们当然不能为了预防死锁把一个临界资源变成非临界资源。
​	**不可抢占条件**是指：进程所获得的资源在未使用完毕之前，不能被其他进程强行抢占 ，而只能由获得该资源的进程自己释放。想要破坏这一条件，我们可以使用 1.3 中提到的**让权等待**原则，即：规定一个进程在提出新的资源请求而不能立即得到满足时，必须释放它已获得的所有资源。这一分配方法叫静态资源分配法。这样的方式存在一些缺点，比如释放资源可能导致前面的工作无效；有的进程可能一直申请不到资源而陷入饥饿状态。
​	**请求和保持条件**是指：进程在等待新资源的同时继续占有已分配的资源。想要破坏这一条件，我们可以规定所有进程都必须一次性申请其在运行过程中所需的全部资源。这一方式也可能有进程饥饿的问题。
​	**环路等待条件**是指：存在一种进程循环链，链中每一个进程已获得的资源同时被下一个进程所请求。想要破坏这一条件，我们可以将系统中的资源按类型赋予不同的序号，并规定所有的进程必须严格按照资源序号递增的顺序申请资源。这一分配方法叫顺序资源分配法。这一方法可能导致不方便增加新的设备、进程使用资源的顺序和编号递增顺序不一致导致的资源浪费、用户程序设计的困难。

​	由上可见，破坏每一种必要条件的方式都存在一定的缺点。为此，我们不如压根就不采用上面的破坏方式，而是在每次资源分配前，检验一下这次分配会不会导致死锁。如果不会则分配；如果会则不分配。这就是死锁的**避免**。我们熟悉的 $Dijkstra$ 先生为了实现这一检验方案，提出了著名的**银行家（$Banker$）算法**。

​	**2.4.3  银行家算法** 

​	银行家算法之所以叫银行家算法，就是因为它模仿的是银行家分发贷款的过程。当且仅当银行家认为某个借贷人可以还得起贷款的时候，银行家才会选择给他分发贷款。这和上面的叙述其实是不谋而合的。

​	**定义（安全状态）** 指存在一种进程序列 $ <P_1, P_2,...,P_n>$，使得操作系统可以按照这个顺序来为每个进程 $P_i$ 分配其所需资源，直至满足每个进程对资源的最大需求，且每个进程都可顺利地完成运行。这一序列叫**安全序列**。如果不存在这样的安全序列，则称系统处于不安全状态。

​	设系统中有 $m$ 类资源，$n$ 个进程，则银行家算法具有这样的数据结构：

~~~
可使用的资源向量 Available[m]
进程最大需求矩阵 Max[n][m]
分配矩阵 Allocation[n][m]
需求矩阵 Need[n][m]
进程 P_i 的请求向量 Requesti[j]
其中：
Need[i][j] = Max[i][j] - Allocation[i][j]
~~~

​	银行家算法的伪码是这样的：

~~~
1. if(Requesti[j] <= Need[i][j]) 转 2；else：错误处理。
2. if(Requesti[j] <= Avalable[i][j]) 转 3；else：P_i等待。
3. 系统试着把资源分配给进程 P_i:
   Available[j] = Available[j] - Requesti[j]; 
   Allocation[i][j] = Allocation[i][j] + Request[i][j];
   Need[i][j] = Need[i][j] - Requesti[j];
4. 系统执行安全性算法，检查此次资源分配后，系统是否处于安全状态。
   if(safety) 分配；
   else：不分配，转未分配情况，阻塞P_i。
~~~

​	上述伪码中涉及到的**安全性算法**具有这样的数据结构：

~~~
工作向量 Work[m]：它表示系统可提供给进程继续运行所需的各类资源数目。执行安全算法开始时，Work[m] = Available[m];
Finish[n]：它表示系统是否有足够的资源分配给进程使之运行完成。开始时先做 Finish[i] = false ；当有足够资源分配给进程时 再令 Finish[i] = true.
~~~

​	安全性算法其实就是找一个安全序列的过程，具体如下：

~~~
从进程集合中找到一个能满足下述条件的进程：Finish[i] = false && Need[i][j] <= Work[j]。如果找到，转2，否则，转3;
执行：Work[j] = Work[j] + Allocation[i][j]; Finish[i] = true; 转1；
If 所有进程的 Finish[i] = true，系统处于安全状态，否则系统处于不安全状态。
~~~

这里解释一下第 $2$ 步：在第 $1$ 步中，我们找到了一个可顺利运行完成的进程，那么这一步指的就是进程 $P_i$ 运行完成之后，要释放自己握有的所有资源，所以 $Work[j]$ 要增加。

​	**2.4.4  死锁的检测与消除** 有的时候，我们不要求在资源分配的时候检测是否会发生死锁，而是在死锁出现之后，及时地解除死锁。对于资源分配这件事，我们可以使用下图这样的**资源分配图**来表示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-18 180635.png" alt="屏幕截图 2023-12-18 180635" style="zoom:33%;" />

​	在上图中，圆圈代表一个进程，方框代表一类资源，方框内的圆圈代表该类资源的一个单位的资源。从进程到资源的有向边为请求边，表示该进程申请一个单位的该类资源；从资源到进程的边为分配边，表示该类资源已有一个资源分配给了该进程。在上图中，设左边方框的资源为 $R_1$，右边方框的资源为 $R_2$，我们可以看到：进程 $P_1$ 已经分得了两个 $R_1$ 资源，请求了一个 $R_2$ 资源；进程 $P_2$ 分得了一个 $R_1$ 资源和一个 $R_2$ 资源，并又请求了一个 $R_1$ 资源。
​	主观理解，在上图中，如果按照 $P_1 \rightarrow P_2$ 分配资源，那么系统不会出现死锁（其实 $P_2 \rightarrow P_1$ 也行）。在这个过程中，$P_1$ 可以获得全部它想要的资源，在使用结束后释放所有正在使用的资源，而 $P_2$ 也能获得请求的资源。当一个进程可以完成获得全部索求资源、使用资源并释放资源这一系列过程时，我们就可以把该进程的入边和出边都删除，代表其任务结束。这就是资源分配图的**简化**。当某个资源分配图的所有边都可以被删除时，我们就称这个资源分配图是**可完全简化**的，反之则称其是**不可完全简化**的。系统会出现死锁状况的充分必要条件是：该系统的资源分配图是不可完全简化的。这就是**死锁定理**。
​	上述死锁定理的算法实现，我们请读者自行思考，在习题中写下你的答案。

​	有关死锁的消除，其实也非常简单，无非两种方法：**剥夺资源**和**撤销进程**。

<div style="page-break-after:always"></div>

## 习题

1. 请自行设计一种进程调度算法。提示：可以针对多用户环境，为每个用户设置一个就绪队列。
1. 有三个进程 $A,B,C$，它们都申请 $\alpha$ 资源。$A$ 申请 $3$ 个，$B$ 申请 $4$ 个，$C$ 申请 $5$ 个。则系统中最少有多少个 $\alpha$ 资源才能保证这三个进程一定不发生死锁？系统中最多由多少个 $\alpha$ 资源能保证这三个进程一定发生死锁？
1. 编写 $C/C++$ 程序，实现银行家算法。
1. 请简要写出通过死锁定理检测系统是否发生死锁的数据结构和算法步骤。

<div style="page-break-after:always"></div>

# 第三章  存储器管理

## 3.1  程序的链接和装入

​	在第二章中，我们从调度客体（作业、进程）的角度了解了处理机调度的高级调度和低级调度，在这一章中，我们将继续从调度主体（内存、资源）的角度来看处理机调度的有关内容。本章内容极其逆天：讲起来枯燥乏味，全是理论；题目却全是计算，还脱不开理论。因此，笔者希望以最简单的语言和概念把本章的内容写出来。
​	在一切开始之前，我们来复习一下存储器。我们知道，存储器是有**层次结构**的。自底向上，分别是：可移动存储介质（软盘、光盘等）、磁盘（用磁记录方式存储二进制的存储器）、磁盘缓存（磁盘的缓冲存储器）、主存储器（CPU 能直接读取的存储器）、高速缓冲存储器（$Cache$）、寄存器。按照这样的层次顺序，读取速度越来越快、存储容量越来越小、单位存储价格越来越高。主存储器以下的部分叫**辅存**（也叫外存），主存储器及以上的部分叫**主存**（也叫内存）。本章主要讨论的是主存的内容。在本节中，我们先从内存的角度看一看进程是怎么被换进内存里的。

​	让我们先从程序说起。写完了的程序想要被 CPU 运行就要装入内存。其经历可以大致参考下图，可见：其首先被编译成目标模块；各模块被链接在一起，成为一个装入模块；由装入程序将其装入内存中运行。程序在内存中运行之后，内存中就出现了它的进程。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-18 190026.png" alt="屏幕截图 2023-12-18 190026" style="zoom:33%;" />

​	**3.1.1 程序的链接** 

​	**定义（相对地址）** 又称逻辑地址、虚地址，是目标模块中的地址形式。简单地说，就是程序自己内部的地址，首址为 0，其余指令中的地址都相对于首地址来编址。
​	**定义（链接）** 链接是将编译后得到的各个目标模块以及所需的库函数连接在一起，形成一个完整的装入模块。链接程序必须将个目标模块中的相对地址和外部调用符号转换成装入模块中的相对地址。根据链接的时间分，链接可以分为三种方式：静态链接、装入时动态链接、运行时动态链接。

​	**静态链接**指的是在程序运行之前，将各目标模块及它们所需的库函数链接成一个完整的装入模块。这也就是上图中所体现出来的链接方式。具体大致如下图所示：

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231218192202002.png" alt="image-20231218192202002" style="zoom:33%;" />

​	**装入时动态链接**指的是在装入一个目标时，若发生一个外部模块的调用事件，则由装入程序去找出相应的外部模块，将它装入内存，并把它链接到调用者模块上。
​	**运行时动态链接**指的是在执行过程中，当发现一个被调用模块还没装入内存时，立即由操作系统找出该模块，将它装入内存，并把它链接到调用者模块上。

​	对于装入或运行时的链接，被链接的共享代码称为动态链接库 $(Dynamic\ Link\ Library,DLL)$。

​	**3.1.2 程序的装入** 

​	**定义（物理地址）** 又称绝对地址、实地址，是内存中存储单元的地址。可直接寻址。显而易见的是，当我们把程序装入内存时，要做相对地址和物理地址的转换。这就是地址映射技术，又叫**重定位**技术。
​	重定位分为静态重定位和动态重定位。**静态重定位**是指，当用户程序装入内存时，由装入程序一次性地完成逻辑地址到物理地址的转换，以后不再转换；**动态重定位**是指，重定位被推迟到程序真正执行时进行。在这种技术中，为了不影响指令的执行速度，在系统中增设一个重定位寄存器，用它来存放程序在内存中的始址。程序执行时，真正访问的内存地址是相对地址与重定位寄存器中的地址相加而成的。

​	链接和装入时，地址的转换情况大致如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-18 193132.png" alt="屏幕截图 2023-12-18 193132" style="zoom:33%;" />

​	程序的装入方式大概有以下几种：绝对装入方式、可重定位的装入方式、动态运行时的装入方式。

​	**绝对装入方式**只适用于事先能够知道程序被装入内存的位置的情况。这样一来，编译程序可直接将程序的符号地址转换成绝对地址，产生使用绝对地址的目标模块，进而链接成使用绝对地址的装入模块。其优点是装入非常简单，缺点则是很难预先知道程序在内存中会被装在什么位置。
​	**可重定位的装入方式**是采用静态重定位的装入方式。当用户程序装入内存时，由装入程序一次性地完成逻辑地址到物理地址的转换，以后不再转换。其优点是不需要硬件支持，缺点则是程序装入内存后不能移动，可能会浪费存储空间。
​	**动态运行时的装入方式**是采用动态重定位的装入方式。在这里，重定位被推迟到程序真正执行时进行。为了不影响指令的执行速度，在系统中增设一个重定位寄存器，用它来存放程序在内存中的始址。程序执行时，真正访问的物理地址是相对地址与重定位寄存器中的地址相加而成的。其优点是允许程序字段在内存中移动，缺点则是需要硬件支持。

<div style="page-break-after:always"></div>

## 3.2  连续分配

​	**定义3.2.1（连续分配）** 为了能将用户程序装入内存，必须为它分配一定大小的内存空间。连续分配方式是最早出现的一种存储器分配方式。连续分配方式为用户的一个程序分配一连串的内存空间，即：程序中代码或数据的逻辑地址相邻，体现在内存分配时物理地址相邻。连续分配的方式有如下几种：单一连续分配、固定分区分配、动态分区分配、可重定位的分区分配。

​	**3.2.2  单一连续分配** 这是最简单的一种存储管理方式，但只能用于单用户、单任务的操作系统中。采用这种存储管理方式时，可把内存分为系统区和用户区两部分，系统区仅提供给操作系统使用，通常是放在内存的低址部分；用户区是指除系统区以外的全部内存空间， 提供给用户使用。其示意图如下：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 085223.png" alt="屏幕截图 2023-12-19 085223" style="zoom:33%;" />

从数据结构的角度考虑，操作系统希望知道每个作业都被分配的具体位置，因此需要一个存储空间说明表。
	我们来做一些思考：当程序结束运行时，其得到的内存终究是要被释放出来的。假设现在有这样的三个被连续分配内存的程序，其大小分别为 $7kb,1kb,5kb$，那么如果现在位于“夹层”的 $1kb$ 的程序要释放内存，而后面所有等待调度的程序所请求的内存都大于 $1kb$，这一块就永远不会被再次使用；如果在该程序释放内存之后，有两个进程分别请求 $0.5kb,0.4kb$ 的内存，且二者得到内存之后，$0.5kb$ 的那个进程很快释放了内存，那么系统中就会出现两个大小分别为 $0.5kb,0.1kb$ 的区域。随着程序分配越来越多，像这样的空闲区域的大小可能会越来越小，数量却可能越来越多。这就是**外碎片**现象。这一现象单纯从分配的角度来看是不可避免的。

​	**3.2.3  固定分区分配** 将整个用户空间划分为若干个固定大小的区域，在每个分区中仅装入一道作业，这样就形成了最早的、也是最简单的一种可运行多道程序的分区存储管理方式。分区的划分有两种方式：等长划分和变长划分。从数据结构的角度来考虑，显然我们需要为每个分区建立一个结构体，显示其大小、起始地址、是否被分配等信息，这就是分区表；另外，在 3.2.2 中提到的存储空间说明表也要建立。示意图如下：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 091412.png" alt="屏幕截图 2023-12-19 091412" style="zoom:33%;" />

毫无疑问的是，这样的直接分配可能导致空间的浪费。例如，一个只需要 $10kb$ 的程序被分配了 $20kb$，这显然是不恰当的。这就是**内碎片**现象。这一现象是可以通过调整分配方式避免的。

​	**3.2.4  动态分区分配** 在装入程序时按其初始要求分配，或者在其执行过程中通过系统调用进行分配或改变分区大小。这种分配方式可以基本解决内碎片的问题。示意图如下：

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231219091926072.png" alt="image-20231219091926072" style="zoom:33%;" />

从数据结构的角度来考虑，我们需要描述空闲分区的空闲分区表和描述已分配分区的已分配分区表。其中空闲分区表的示意图如下：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 092213.png" alt="屏幕截图 2023-12-19 092213" style="zoom:33%;" />

拍脑袋想，这是一个很简单的过程。当有程序请求分配内存时，先在空闲分区表中寻找是否有满足条件的表项。若有，分配，调整空闲分区表的表项（正好占满则删除表项，未占满则改变分区大小和始址），添加已分配分区表的表项；当有程序请求释放内存时，删除已分配分区的表项，调整空闲分区表的表项（前后有空则合并，前后无空则添加表项）。
	在这种分配方式中，有关被分配内存的寻找，存在两大类算法：基于顺序搜索的算法和基于索引搜索的算法。

​	基于顺序搜索的算法包括：首次适应算法、循环适应算法、最佳适应算法和最坏适应算法。
​	**首次适应算法（First Fit）**要求空闲分区表以地址递增的次序连接，每次从链首开始顺序查找，把最先找到的满足需求的空闲区分配之，这一算法的目的在于尽量减少查找时间。
​	**循环适应算法（Next Fit）**将空闲分区表做成一个环形链表，每次分配从上次找到的空闲分区的下一个空闲分区开始查找能满足需求的空闲区。这一算法可以在一定程度上避免低址部分留下很多很小的空闲分区。
​	**最佳适应算法（Best Fit）**从全部空闲区中找出能满足作业需求的容量最小的空闲区分配之，此法的着眼点是使碎片尽量小。为了加速寻找，该算法要求将所有的空闲分区按其容量以从小到大的顺序形成一空闲分区表。
​	**最坏适应算法（Worst Fit）**总是挑选一个最大的空闲区，从中分割一部分存储空间给作业使用。此法的目的在于使剩下的空区最大，减少空区碎片机会。该算法要求将所有的空闲分区按其容量以从大到小的顺序形成空闲分区链。

​	基于索引搜索的算法包括：快速适应算法、伙伴系统算法、哈希算法。
​	**快速适应算法（Quick Fit）**按照空闲分区的大小建立索引。对于每一类具有相同大小的所有空闲空间分区，单独设立一个空闲分区链表。同时，在内存中设立一张索引管理表，每个索引表项对应一种空闲分区大小。
​	**伙伴系统算法（Buddy System）**是快速适应算法的改进策略。在伙伴系统中，分区大小均为 $2$ 的 $k$ 次幂，系统刚开始运行时，整个内存区是一个大小为 $2^m(m>k)$ 的空闲分区，在系统运行过程中，由于不断地划分，将形成若干个不连续的空闲分区。对不连续的空闲分区，按分区大小进行分类。对具有相同大小的所有空闲分区，单独设立一个空闲分区双向链表。
​	分配时，要做以下事情：设需分配长度为 $n$ ，计算一个 $i$ 值，使得 $2^{i - 1} < n \le 2^i$，在大小为 $2^i$ 的空闲分区链表中查找。若找到，则分配给进程。若无 ，在 $2^{i + 1}$ 的空闲分区链表中查找。若找到，且把它均分两块，称为“伙伴”， 一个加入 $2^i$ 分区链，一个分配。如果大小为 $2^{i + 1}$ 的空闲分区也不存在，则继续查找大小为 $2^{i +2}$ 的分区，若找到，则对其进行两次分割，第 1 次将其分为大小为 $2^{i + 1}$ 的两个分区，一个用于分配，一个加入到大小为 $2^{i + 1}$ 的空闲分区链表中；第 2 次，将第 1 次用于分配的空闲区分割为 $2^i$ 的两个分区，一个用于分配，一个加入到 $2^i$ 的空闲分区链表中，以此类推。同样地，回收时，被回收块也要找自己的“伙伴”，所以可能出现多级合并。
​	**哈希算法（Hash）**利用哈希快速查找的优点，以及空闲分区在可利用空闲区表中的分布规律，建立哈希函数，构造一张一空闲分区大小为关键字的哈希表，该表的每一个表项记录了一个对应的空闲分区链表。

​	虽然上述算法实现了动态的分区分配，但是却没有解决外碎片的问题。实际上，我们拍脑袋想，想要解决外碎片的问题，无非是靠移动已分配的内存从而实现空闲碎片的集聚，这一操作叫**紧凑**（也叫拼接）。实现了紧凑操作的分配方式就是可重定位的分配方式。

​	**3.2.5  可重定位的分区分配** 可重定位分区分配方式就是在动态分区分配方式的基础上增加了紧凑功能。由于紧凑时，作业需要在内存中移动位置，需要动态重定位技术的支持，因此该方式又被称为动态重定位分区分配。其示意图大致如下：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 101605.png" alt="屏幕截图 2023-12-19 101605" style="zoom:33%;" />

​	**3.2.6  分区的保护** 在连续分配方式中，为了防止一个用户作业破坏操作系统或其他用户的作业，常采用界限寄存器或保护键的方法来进行分区的保护。
​	**界限寄存器**可以是一对上、下限寄存器，也可以是一对基址、限长寄存器。每当进行内存访问时，硬件自动将所访问的内存地址与界限寄存器的值进行比较，若发生地址越界，则产生越界中断。
​	**保护键**为每个分区上了一个锁；同时为每个进程分配一个相应的钥匙。保护键可设置成对读写同时保护的或只对读、写进行单项保护的。 当进行内存访问时，都要检查钥匙和被访问单元的锁是否匹配，若不匹配，则发生保护性中断。保护键的一个例子如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 102447.png" alt="屏幕截图 2023-12-19 102447" style="zoom: 33%;" />

​	**3.2.7  对换** 多个程序并发执行时，可以将暂时不能执行的程序送到外存中，从而获得空闲内存空间来装入新程序或读入保存在外存中而目前到达就绪状态的进程。交换单位可以为整个进程的地址空间（进程对换）也可以为进程的部分地址空间（页面对换、分段对换）。其原理是：暂停执行内存中的进程，将整个进程的地址空间保存到外存的交换区中（换出，$swap-out$）；将外存中由阻塞变为就绪的进程的地址空间读入到内存中，并将该进程送到就绪队列（换入，$swap-in$）。

​	这项技术的优点是：增加并发运行的程序数目、给用户提供适当的响应时间、编写程序时不影响程序结构；缺点是：对换入和换出的控制增加处理机开销、没有考虑执行过程中地址访问的统计特性。

<div style="page-break-after:always"></div>

## 3.3  分页存储管理方式

​	**3.3.1  分页原理** 在 3.2 中，我们学习了连续分配的方式，即：为用户的一个程序分配一连串的内存空间。实际上，我们完全可以采取分治的思想，把一个程序的存储分为一个个小小的部分，把内存也分成一个个等大小的小部分，从而把进程离散地放入内存之中。我们把程序分成的小部分成为**页（page）**，物理内存分成的小部分称为**块（block）**，这一存储方式就叫**分页存储管理方式**。其示意图如下。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 103636.png" alt="屏幕截图 2023-12-19 103636" style="zoom: 33%;" />

​	我们来继续拍脑袋想一些事情：首先，从数据结构的角度考虑，为了管理这些页，我们自然需要一个**页表**。页表描述的是每个进程所对应的物理块，实现了相对地址到物理地址的映射，就像上图中位于中间的那个表一样（当然，其实我们根本不需要页号那一栏，因为块号的下标其实就是页号）。对于任意的一条指令或者数据，其自身的相对地址 $addr$ 就可以表示为`页号＋页内偏移量`。设页面大小为 $L$，页号为 $p$，页内偏移量为 $N$，则以下公式成立：
$$
p = \lfloor \frac{addr}{L} \rfloor \\
N = addr \ \%\  L \\
addr = p \times L + N
$$
显然，页内的偏移量必然和块内的偏移量是相等的。这样一来，如果我们知道一个数据存放始址的`页号＋页内偏移量`，我们就可以通过页表由页号找到块号，再由偏移量找到数据或指令的始址内容。这里请读者注意，在做题时，千万不要把**数据地址的格式**和**页表的格式**弄混。数据地址是``页号＋页内偏移量``，描述的是一个地址；页表格式是``页号＋块号``，描述的是页到块的映射。
	除此之外，整个系统有一个物理页面表，描述物理内存空间的分配使用状况。描述物理页面表的数据结构自然是可以使用链表的；系统中还有一个请求表，描述系统内各个页表的位置和大小，用于地址转换。

​	接下来就让我们把拍脑袋想出来的东西交给操作系统，看看操作系统是怎么处理的。

​	**3.3.2  分页存储管理系统的地址变换机构** 地址变换机构可以借助进程的页表将逻辑地址转换为物理地址。基于地址变换机构的内存访问过程大致如下：

~~~
首先，得到相对地址 addr，计算页号 p 和页内偏移量 N；
比较页号 p 和页表长度 m，若p>=m，则产生越界中断，否则继续执行。注意：页号是从 0 开始的，而页表长度至少是1，因此p == m时也会越界。
在页表中找到这个页表项，并取出内存块号。
计算物理地址，用得到的物理地址 E 去访存。
~~~

其示意图如图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 145212.png" alt="屏幕截图 2023-12-19 145212" style="zoom:33%;" />

​	我们从计算机组成原理课程中学到：存储器的访问是有**局部性原理**的。即：被访问过的数据不久后很可能再次被访问；被访问的存储单元附近的单元不久后很可能被访问。为此，我们可以把刚刚被访问过的页表项放进一个存储结构中，在下一次访问时，先查看该结构中的内容，如果找到了，就直接取用；如果没找到，再查看页表。这一结构称为**快表（TLB）**，又叫联想寄存器。在快表中找到了所需要的内容叫**快表命中**。快表的更新有很多种算法，最简单的是先进先出算法（First in First Out，FIFO）。由此，我们可以设计出一个**具有快表的地址变换机构**，如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 150108.png" alt="屏幕截图 2023-12-19 150108" style="zoom:33%;" />

​	我们把从进程发出指定逻辑地址的访问请求，经过地址变换，到在内存中找到对应的实际物理地址单元并取出数据，所需要花费的总时间成为**内存访问有效时间（Effective Access Time，EAT）**；把使用快表并在其中成功查找到所需页面的表项的比率叫**快表命中率**。设访问内存的时间为 $t$，表命中率为 $a$，快表更新的时间为 $\lambda$ ，则在没有引入快表时，$EAT$ 的计算方式为：
$$
EAT = t + t = 2t
$$
在引入快表之后，EAT 的计算方式为：
$$
EAT = a \times \lambda \ + \ (1 - a)(\lambda\ +\ t \ + \lambda) + t = (2 - a)(\lambda \ -\  t)
$$
​	

​	**3.3.3  多级页表** 当虚拟地址空间很大而每页比较小时，进程页表会很长。这时，为了方便查找，我们可以采用两级或多级页表。在两级页表时，指令或数据的地址分为三部分：页表号、页号和偏移量。多级页表的结构大致如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 153049.png" alt="屏幕截图 2023-12-19 153049" style="zoom:33%;" />

对于存在快表的多级页表系统，只有第一级页表（最内层页表）的表项可以放到快表之中。

​	**3.3.4  反置页表（Inverted Page Table，IPT）** 一般页表的表项是按页号进行排序，页表项中的内容是物理块号。而反置页表是为每一个物理块设置一个页表项并将按物理块号排序，其中的内容则是页号及其隶属进程的标志符。在利用反置页表进行地址变换时，用进程标志符和页号去检索反置页表。若检索完整个页表都未找到与之匹配的页表项，说明此页此时尚未调入内存，应产生请求调页中断或地址出错；如果检索到与之匹配的表项，则该表项的序号便是该页所在的物理块号，该块号与页内地址一起构成物理地址。
​	反置页表的表项一般成千上万。为此，我们可以设置多级反置页表或者哈希表来方便检索。

<div style="page-break-after:always"></div>

## 3.4  分段存储管理方式

​	**3.4.1  分段原理** 分段系统和分页系统是很相似的，都是把程序的代码段和数据段分成若干部分。只不过，在分页系统中，我们不用考虑**“怎么分”**的问题，但是在分段系统中是要考虑的。简单地说，分段系统就是把程序分为若干个**段（Segment）**，如主程序段 MAIN、子程序段 X、数据段 D 和栈段 S 等。每个段都有自己的名字，通常可用一个段号来代替段名。段内从 0 开始编址，并采用一段连续的地址空间。各段的长度一般是不等的。各段之间在内存的分布可以是离散的。这样一来，对于任意的一条指令或者数据，其自身的相对地址 $addr$ 就可以表示为`段号＋段内偏移量`。

​	我们来继续拍脑袋想，当然地，我们要给各个段建立一张**段表**，每个段在表中占有一个表项，记录了该段在内存中的始址（基址）和段长度，从而实现相对地址到物理地址的映射。其结构示意图如下。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 160707.png" alt="屏幕截图 2023-12-19 160707" style="zoom:25%;" />

​	**3.3.2  分段存储管理系统的地址变换机构** 这部分内容和分页的地址变换机构比较类似。

~~~
首先，得到段号 p 和段内偏移量 N；
比较段号 p 和段表长度 m，若p>=m，则产生越界中断，否则继续执行。
在段表中找到这个段表项。
根据基址和偏移量，用得到的物理地址 E 去访存。
~~~

其示意图大致如下：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 180334.png" alt="屏幕截图 2023-12-19 180334" style="zoom:25%;" />

​	

​	**3.3.3  段页式存储管理方式** 段页式系统的基本原理是分段和分页原理的结合。先将用户程序分成若干个段，再把每个段分成若干个页。其示意图大致如下。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 180811.png" alt="屏幕截图 2023-12-19 180811" style="zoom:25%;" />

其中，任意的一条指令或者数据的相对地址可以表示为：`段号+段内页号+页内偏移量`。由此可见，在这种存储管理方式中，我们同时需要段表和页表，而且段表的每一项都有一个页表。在段表中，我们需要存储段号、页表大小、页表始址；在页表中，我们需要存储页号和物理块号。这样一来，我们就实现了段页式存储管理方式的地址映射。大致如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 182138.png" alt="屏幕截图 2023-12-19 182138" style="zoom:25%;" />

​	**3.3.4  分段的共享与保护** 你可能很疑惑的是，为什么在有了分页存储管理方式之后，还要研究分段存储管理方式？事实上，分段系统的优点就在于：容易实现代码和数据的共享和保护。
​	先谈**共享**，多个段可能可以共享同一个函数或者同一段代码，这就可以通过一个**共享段表**来实现。如下图所示。

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231219213856022.png" alt="image-20231219213856022" style="zoom:33%;" />

​	上图中，段名那一行是该共享段的性质，我们可以把它简称为“段行”；状态那一行是一个共享该段的进程的性质，可以简称为“进程行”。省略号的意义是对于其中任意一个新增的共享进程，都要添加一栏该进程的进程行。接下来，我们来详细解释一下三个表项：共享进程计数、存取控制字段和段号。

~~~
共享进程计数：记录共享这一段的进程的个数。当其值为 0，说明没有进程需要它了。这时就可以将这一段所占用的空间释放出去了。
存取控制字段：对于一个共享段，应当为不同的进程设置不同的读写权限。其权限分为三类：只读、只运行、可读写。
段号：一个共享段在不同的进程中可以有不同的段号，每个进程可以使用不同的段号访问共享段。
~~~

​	再说**保护**，由于每个分段在逻辑上是相互独立的，所以比较容易实现信息保护。我们可以通过三种措施来确保信息的安全：越界检查、存取控制检查、环保护机构。

​	越界检查是利用地址变换机构完成的。段的越界检查除了要检查段号是否越界之外，还要检查段长是否越界。这可以确保每个进程都只在自己的地址空间运行。
​	存取控制检查是基于硬件实现的。不允许只读用户的写请求和只运行用户的读请求，从而保证信息安全。
​	环保护机构是一种功能较为完善的保护机制。这一机制规定：低编号的环具有高优先权，所以操作系统的核心处于 0 号环内；某些重要的实用程序和操作系统服务占中间环；一般的应用程序则被放在外环上，如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-20 144114.png" alt="屏幕截图 2023-12-20 144114" style="zoom:33%;" />

其原则是：一个程序可以访问本环和外环中的数据，可以调用本环和内环中的服务。

<div style="page-break-after:always"></div>

## 3.5  虚拟存储器

​	作为本节的引入，我先来讲一讲我的一次具体经历：我在 $Microsoft\ Xbox$ 上下载过一个 $98GB$ 的游戏。当我下载不到 $50\%$ 的时候，我手欠地点击了“运行游戏”，结果游戏却成功运行了！我甚至在这种下了一半的情况下通关了游戏的第一章。也就是说，对于这个游戏而言，如果我们仅仅是想让它运行，我们并不需要把它的一切组件都下载下来，只需要下载那些使它``暂时``能够运行的组件即可；同样地，对于一个进程而言，我们想要其运行，**并不需要把其所有的页（或段）调入内存，只需要把当前正在执行的页（或段）调入内存即可。**这就是虚拟存储器的原理。

​	**定义3.5.1（虚拟存储器）** 所谓虚拟存储器，是指具有请求调入功能和置换功能、能从**逻辑上**对内存容量加以扩充的一种存储器系统。在虚拟存储器系统中的程序运行时，只把其当前执行的页（或段）调入内存。如果出现**缺页**（或缺段），即希望运行的页（或段）不在内存中，则由处理器通知操作系统将相应的页或段调入到内存，然后继续运行程序；内存中暂时不使用的页（或段）会被调出，保存在外存上，从而腾出内存空间存放将要装入的程序以及将要调入的页（或段）。
​	虚拟存储器的特征有三方面：多次性、对换性、虚拟性。**多次性**是指其将一个作业分成多次调入内存；**对换性**是指其允许将那些暂不使用的程序或数据从内存调至对换区，待以后需要时再调入内存，从而能有效地提高内存利用率；**虚拟性**是指其对内存的扩充是逻辑上的，用户所看到的大容量只是一种感觉，并不是实际存在的。

​	使用虚拟存储器的分页、分段存储管理方式分别叫**请求分页存储管理方式**和**请求分段存储管理方式**。

​	**3.5.2  请求分页存储管理方式** 在简单页式存储管理的基础上，增加请求调页和页面置换功能。这样一来，页表的字段就需要扩充。如下图所示，请求分页存储管理方式在原来的基础上添加了状态位 $P$（标识是否在内存）、访问字段 $A$（记录字段在一定时间内被访问的次数，用于被调换页面的选取）、修改位 $M$（标识是否修改）、外存地址。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 185037.png" alt="屏幕截图 2023-12-19 185037" style="zoom:25%;" />

另外，在操作系统中，还要添加一个**缺页中断机构**。当所要访问的页面不在内存时，便产生缺页中断，将所缺页调入 内存。需要解释一下的是，缺页中断在指令执行期间产生和处理，而不是在一条指令执行完毕之后。因此，一条指令的执行或者一个数据的读取都可能经历多次缺页中断。
	扩展性地解释一下：我们当然可以把那些预计在不久之后便会访问到的几个页预先调入内存，这就是**预调页策略**。但是虽然有局部性原理，预测这个事依然是很困难的。因此老老实实地按照请求分页方法通过缺页中断调页还是很不错的。

​	请求分页系统中的地址变换过程大致如下图所示。这是一个具有快表的系统。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 190114.png" alt="屏幕截图 2023-12-19 190114" style="zoom:33%;" />

​	毫无疑问的是，我们调出某一页的策略是有一个好坏标准的，定量衡量这一标准的就是**缺页率**。这也很好理解，缺页中断会导致内外存的交换，这和程序顺序执行相比必然是更费时间的。缺页率的定义为：
$$
缺页率 = \frac{缺页次数}{总的页面访问次数}
$$
​	影响缺页率的因素有：页面大小（越大，缺页率越低）、分配给进程的物理块数目（越多，缺页率越低）、页面的置换算法等。
​	针对上面提到的“页面置换算法”，我们做一些拍脑袋的思考：当我们需要从内存中调出一页时，应当调出哪页呢？容易理解的是，根据访问的**局部性原理**，最近访问过的页面在短期内容易被再次访问，因此我们可以针对访问字段 $A$ 做一个贪心算法（贪 $A$ 值小的）；另外，当这一页被修改之后再被换出的时候，我们需要把这一页写回外存，这就招致了时间浪费，不应该是我们换出页时的首选项。

​	**3.5.3  请求分页系统中的页面置换算法** 我们在这里介绍 $7$ 个页面置换算法：最佳置换算法、先进先出置换算法、最近最久未使用算法、最少使用算法、$Clock$ 置换算法、改进型 $Clock$ 置换算法、页面缓冲算法。我们将使用这样的一个例子来解释上述算法：假定系统为某进程分配了 $3$ 个物理块， 并考虑有以下的页面号引用：
$$
7，0，1，2，0，3，0，4，2，3，0，3，2，1，2，0，1，7，0，1
$$
​	**最佳置换算法**是一个几乎不可能实现的算法，它要求知晓未来的物理块访问流程。其所选择的被淘汰页面，将是以后最少使用的页面。对于上述引用，如果我们提前知晓整个访问的流程，其结果应当如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 193559.png" alt="屏幕截图 2023-12-19 193559" style="zoom:33%;" />

​	**先进先出页面置换算法（FIFO）**我就不多解释了。直接上图。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 193824.png" alt="屏幕截图 2023-12-19 193824" style="zoom:33%;" />

​	**最近最久未使用算法（LRU）**是一种贪心算法，贪的是很长时间没有被访问过的页面。如图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 200618.png" alt="屏幕截图 2023-12-19 200618" style="zoom:33%;" />

​	**最少使用算法（LFU）**也是一种贪心算法。正如之前我们所思考的，贪的是“访问字段”中最少的那个。可以理解的是，在采用 LFU 算法时，对一个新调入的页面，可能会因为它的访问次数最少而被淘汰，而另一些页面因为在某个时候被访问多次，即使以后不再使用，也不会被淘汰，此时， LFU 算法将会变得十分低效。请读者尝试自行画出其页面换入换出的结果。

​	**$Clock$ 算法**又叫最近未使用算法，是 LRU 算法的一个近似算法。在这一算法中，我们把每页的访问位设置为一个二值的布尔量，再将内存中的所有页面都通过链接指针链接成一个循环队列。当某页被访问时，访问位置 $1$。置换算法在选择一页淘汰时，只检查页的访问位，如果是 $0$ 就选择该页换出；如果是 $1$ 则将它置 $0$，然后继续检查。当检查到队列中的最后一个页面时，若访问位仍为 $1$，则循环返回到队首去检查第一个页面。这样一来，假设物理块有 $n$ 块，其最多检查 $n + 1$ 次就能找到应当换出的页面。请读者尝试自行画出其页面换入换出的结果。

​	**改进型  $Clock$ 算法**同时考察访问位$(A)$和修改位$(M)$。正如之前我们所思考的，我们最好不要换出已被修改过的页面。这样一来，访问位和修改位就会有四种布尔组合：$00,01,10,11$。其执行步骤如下：

1. 从指针所指示的当前位置开始，扫描循环队列，寻找第一个 $A=0$ 且 $M=0$ 的页面作为淘汰页 。在第一次扫描期间不改变访问位 $A$。
2. 如果第一步失败，则开始第二轮扫描，寻找 $A=0$ 且 $M=1$ 的页面，将所遇到的第一个这类页面作为淘汰页。在第二轮扫描期间，将所有扫描过的页面的访问位都置 $0$。
3. 如果第二步也失败，则将指针返回到开始的位置，然后重复第一步；如果仍失败，再重复第二步，此时就一定能找到被淘汰的页。

​	**页面缓冲算法（PBA）**是 FIFO 算法的发展。在换页时还是使用 FIFO 算法，只不过在换出页面时，不是将其直接调回外存，而是设置一个缓冲池，将其调入缓冲池里。这样一来，当刚换出的页面再次被访问时，可以从缓冲区找回该页面。为了减少外存访问次数，我们可以在缓冲池里设置两个链表。一个是页面修改位为 0 的`空闲页面链表`，一个是页面修改位为 1 的``已修改页面链表``。如果页面未被修改，就将其放到空闲页面链表的末尾；否则将其放到已修改页面链表的末尾。当已修改页面达到一定数目后，再将它们一起调出到外存，然后再将它们归入空闲页面链表。这样有几率大大减少访问外存的次数。

​	与基本分页存储管理方式不同的是，在请求分页管理方式中，内存有效访问时间不仅要考虑访问页表和访问实际物理地址数据的时间，还必须要考虑到缺页中断的处理时间。设 $\lambda$ 为访问和更新快表的时间，$t$ 为访问内存的时间，$\epsilon$ 为缺页中断处理时间，则在请求分页系统中，存在三种**内存访问有效时间** $EAT$：

1. 页在内存，且快表命中时，$EAT = \lambda + t$；

2. 页在内存，而快表未命中时，有效访问时间分为四大块：查找快表时间、查找页表时间、更新快表时间和访问实际物理内存时间。所以
   $$
   EAT = \lambda + t + \lambda + t = 2(\lambda + t)
   $$

3. 页面不在内存时，有效访问时间分为五大块：查找快表时间、查找页表时间、缺页中断处理时间、更新快表时间 和访问实际物理内存时间。所以
   $$
   EAT = \epsilon + 2(\lambda + t)
   $$
   如果考虑概率，设 $a$ 为快表的命中率，$f$ 为缺页率，则：
   $$
   EAT & = \lambda + at + (1-a)[t + f (\epsilon + \lambda + t) + (1 - f)(λ + t) \\
   & =\lambda + (1 - a)(t + f\epsilon + \lambda) + t
   $$
   而如果不引入快表，则不存在快表的命中率 $a$，上述公式则会变成：
   $$
   EAT = t + f(\epsilon + t) + (1 - f)t
   $$

上述三种 $EAT$ 都可能在考试中出现，请注意理解。

​	接下来，我们来对请求分页系统的性能做一些简单的分析。

​	**定义3.5.4（抖动）** 在系统中运行的进程太多，分配给每一个进程的物理块太少，不能满足进程正常运行的基本要求，就会导致每个进程在运行时，频繁地出现缺页。这样一来，对磁盘的有效访问时间急剧增加，各个进程的大部分时间都用于页面的换进换出，而不能再去做任何有效的工作，处理机的利用率就会急剧下降，并趋于 0 。这种情况就叫抖动。当抖动发生时，我们可以极端地直接暂停部分进程。不过，我们当然不希望抖动发生。为了预防抖动，$Denning$ 提出了**工作集**的概念。
​	**定义（工作集）** 在某段时间里，进程实际所要访问页面的集合。虽然程序只需要少量的几页在内存便可运行，但为了较少地产生缺页，应该将程序的全部工作集装入内存中。正如之前所提到的，我们无法事先预知程序在不久的将来将访问哪些页面，因此还要像置换算法那样，用程序的过去某段时间内的行为近似之后的行为。记一个进程在时间间隔 $t$ 内的工作集为 $w(t,\delta)$，其中 $\delta$ 表示工作集的大小，也叫窗口大小。下图中的进程将要访问如左侧序列中的页面，其中窗口大小分别为 $3,4,5$。其进程的工作集如右侧所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 210753.png" alt="屏幕截图 2023-12-19 210753" style="zoom:33%;" />

​	**3.5.5  请求分段存储管理方式** 请求分段系统中所需的支持部件有段表、缺段中断机构，以及地址变换机构。其中段表的表项结构除了添加存在位、访问字段和修改位，还添加了存取方式（执行、只读、可写）、增补位（此段是否增长），大致示意图如下：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 212430.png" alt="屏幕截图 2023-12-19 212430" style="zoom:25%;" />

​	缺段中断机构与缺页中断机构类似，它同样需要在一条指令执行期间，产生和处理中断，以及一条指令执行期间可能产生多次缺段中断。与缺页中断不同的是，由于分段是信息的逻辑单元，因而不可能出现一条指令被分割在两个分段中和一个信息被分割在两个分段中的情况。缺段处理的大致流程如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 211227.png" alt="屏幕截图 2023-12-19 211227" style="zoom:33%;" />

请求分段系统中地址变换机构是在分段系统地址变换机构的基础上形成的。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 211431.png" alt="屏幕截图 2023-12-19 211431" style="zoom:33%;" />

<div style="page-break-after:always"></div>

## 习题

1. 某系统采用页式存储管理策略，拥有逻辑空间 $32$ 页，每页大小为 $2KB$ ，拥有物理空间 $1MB$。
   1. 写出逻辑地址的格式；
   2. 若不考虑访问权限，进程的页表有多少项？每项至少有多少位？
   3. 如果物理空间减少一半，页表结构应相应作怎样的改变？

2. 已知某分页系统主存容量为 $64KB$， 页面大小为 $1KB$ ，对一个 $4$ 页的作业，将第 $0,1,2,3$ 页分配给主存的 $2,4,6,7$ 号块。试将十进制的虚拟地址 $1023,2500,4500$ 转换成物理地址。

3. 一台机器有 $48$ 位虚地址和 $32$ 位物理地址，若页长为 $8KB$，问页表共有多少个页表项？如果设计一个反置页表，则有多少个页表项？

4. 对于下图的段表，请将逻辑地址 $(0,137),(1,4000),(5,230)$ 转换为物理地址。

   <img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-19 180614.png" alt="屏幕截图 2023-12-19 180614" style="zoom:33%;" />

5. 某虚拟存储器的用户空间共有 $32$ 个页面 每页 $1KB$，主存 $16KB$ 。 假定某时刻系统为用户的第 $0,1,2,3$ 页分配的物理块号为 $5 ,10, 4, 7$，而该用户作业的长度为 $6$ 页，试将十六进制的虚拟地址 $0 A 5 C 、 103 C 、 1 A 5 C$ 转换成物理地址。

6. 在一个请求分页系统中，假如一个作业的页面走向为 $4，3，2，1，4，3，5，4，3，2，1，5$，目前它还没有任何页装入内存，当分配给该作业的物理块数目 $M$ 为 $4$ 时，请分别计算采用 OPT，LRU 和 FIFO 页面淘汰算法时访问过程中所发生的缺页次数和缺页率。

<div style="page-break-after:always"></div>

# 第四章  文件管理

## 4.1  文件概述

​	在现代计算机系统中，系统和用户需要用到的大量程序和数据被以文件的形式存放在外存中。为了管理这些文件，操作系统增加了文件管理的功能，把对文件的多种操作手段提供给用户，这就是操作系统中的**文件系统**。以我们平常接触的 $Windows$ 操作系统上的文件来考虑，容易理解的是，我们对文件的操作会包括：创建文件、删除文件、打开文件、关闭文件、读写文件、读写指针定位等。操作系统当然就是为了实现这些功能而存在的。以**打开和关闭文件**为例，在打开文件时，操作系统将指名文件放到一个表目中（该表目记载了所有已打开的文件），并将该表目的编号返回给用户；在关闭文件时，则是把其从表目中删除。
​	操作系统中的文件系统是有层次结构的。自底向上，分别为对象层、操作层、接口层。对象层描述了文件系统控制的对象及其属性，对象主要包括文件、目录和磁盘的存储空间（外存）；操作层描述了对对象操纵和管理的软件集合，是实现文件系统功能的基础；接口层实现了用户与文件系统的接口。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片1.jpg" alt="图片1" style="zoom: 50%;" />

​	**4.1.1  文件及其逻辑结构** 

​	**定义（文件）** 文件是指由创建者所定义的、具有文件名的一组相关数据元素的集合。容易理解的是，无论是什么文件，都有一种在存储器上的组织形式，而这种组织形式是用户看不见的。这就叫文件的**物理结构**。相对地，用户能够通过操作系统调用而完成读写的文件都是**逻辑文件**，我们当然希望逻辑文件具有良好的性质，使得检索速度加快、存储空间占用少等。因此，逻辑文件应当具有一定的**逻辑结构**。
​	逻辑文件按照是否有结构来分，有有结构文件（记录式文件）和无结构文件（流式文件）两种。**有结构文件**由记录组成，而记录由数据项组成；**无结构文件**以字节为单位，可以认为其一个字节就是一个记录。很多源程序、可执行文件、库函数等都是无结构文件；$UNIX$ 系统中的所有文件都是无结构文件。

​	**4.1.2  有结构文件及其寻址**  正如上一段提到的，有结构文件由记录组成，而记录由数据项组成。**数据项**是最低级的数据组织形式。数据项分为基本数据项和组合数据项，其中组合数据项为基本数据项组合而成的。例如学生的学业成绩是各组合数据项，而数学成绩、语文成绩、英语成绩则是其中的基本数据项。**记录**是一组数据项的集合。其包含的数据项描述了它描述对象的什么方面。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-05 104704.png" alt="屏幕截图 2023-12-05 104704" style="zoom: 33%;" />

​	那么有结构文件都有哪几种结构呢？按照文件的组织方式，我们可以将其分为：顺序文件、索引文件、索引顺序文件和哈希文件。

​	**4.1.2.1  顺序文件**  顺序文件是最基本、最简单的有结构文件，即：记录按一定顺序进行排列。顺序文件的结构一般有串结构和顺序结构两种。**串结构**的理念就是按照时间存入先后对文件排序，查找某一文件时必须从头开始；**顺序结构**则是通过指定一个关键字来排序，从而可以实现折半查找、跳跃查找等省时的查找方案。顺序文件的优点是适合批量存取和磁带存储，缺点则是单个数据查找开销较大、增删记录比较困难。
​	**顺序文件的记录寻址**  为了访问顺序文件的一条记录，首先应该找到该记录的地址。查找地址的方法分为隐式寻址和显式寻址两种。隐式寻址就是在读到第 $i$ 个记录时，获取这一记录的长度 $L_i$，从而在本地址 $p_i$ 的基础上跳到 $p_i + L_i$ 去读下一个记录；显式寻址以一个整数 $k_i$ 来表示一个记录。如果要查找第 $i$ 个记录，直接从首地址出发，计算到该记录的 $\sum_{j= 0} ^{i - 1} k_{j}L_{j}$，编程实现必然是简单的，但是实际操作显然是麻烦的，尤其是遇到变长记录的时候。

​	**4.1.2.2  索引文件**  如果我们为变长记录文件建立一张索引表，为主文件中的每个记录在索引表中设置一个表项，记录指向该记录的指针和记录的长度 $L_i$，索引表按照一个指定的关键字排序，那么，在索引表上的文件查找就是把变长查找变成了等长查找。如下图 $(a)$ 所示。另外，带有关键字的查找是可以使用算法的。这也进一步缩短了查找时间。

<img src="C:\Users\21117\Pictures\Typora\图片2.jpg" alt="图片2" style="zoom:50%;" />

​	容易理解的是，在实际使用中，处于不同的目的，可能会按照不同的属性（关键字）来检索一条记录。为此，我们可以通过多个关键字建立多个索引表。如上图 $(b)$ 所示。

​	**4.1.2.3  索引顺序文件**  索引顺序文件是顺序文件和索引文件的结合，是最常见的一种逻辑文件形式。它保留了顺序文件由关键字组织的特征，同时引入了文件索引表，从而实现文件的随机访问；增加了溢出文件，用于记录新增的和删除的记录。
​	最简单的索引顺序文件只建立了一级索引。首先，将所有记录分为若干个组，然后为顺序文件建立一张索引表，为每组中的第一个记录在索引表中建立一个索引项，其中含有该记录的关键字和指向分组的指针。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片3.jpg" alt="图片3" style="zoom: 50%;" />

类似地，可以建立两级索引、多级索引。

​	**4.1.2.4  哈希文件**  使用哈希$(Hash)$函数将关键字转换为相应记录的地址。

<div style="page-break-after:always"></div>

## 4.2  文件目录

​	4.1 中，我们从文件自身的角度了解了文件的逻辑结构、文件的存储与寻址。接下来，我们来从操作系统的角度看一看文件的管理。首先，为了知道文件的信息，必须存在这样的一种数据结构，将文件自身的文件名、扩展名、物理地址、逻辑结构等标识出来。这就是**文件控制块**（FCB）；然后，为了统筹文件的情况，必须存在这样的一种数据结构，使得许许多多的 FCB 能够排列出来，这就是**文件目录**。

​	**定义4.2.1（文件目录）** 用于标识系统中文件及其物理地址的一种数据结构，供检索使用。文件目录的管理具有这样的要求：可按文件名实现搜索（按名存取）、检索速度高、允许文件共享、允许文件重名。直观上来看，按名存取和允许文件重名这两点似乎是存在矛盾的。事实上，文件目录可以理解为 $Windows$ 系统下的文件夹。不同文件夹下的文件自然是可以重名的。
​	文件目录通常都是放在磁盘上的。当文件很多时，文件目录可能要占用大量的盘块。在操作系统的**按名**查找过程中，如果一直没有找到文件，就需要持续地将盘块调入内存。这可能要消耗大量的时间。为此，我们接着来拍脑袋想一些事情：按名检索的过程中，我们只使用了文件的名字，而其他的描述信息一概不用，那么这些信息自然是可以不调入内存的。这就引入了**索引结点($i\_ node$ 结点)**，其原理就是：将文件名和文件信息分开，使文件信息单独形成一个数据结构，该结构就是索引结点。我们只把文件名和指向索引结点的指针作为一个文件目录的表项调入内存。

​	**4.2.2  文件目录的组织结构** 拍脑袋想，文件目录的结构无非两种：单级目录和多级目录。

​	**单级目录**是最简单的文件目录。在整个文件系统中只建立一张目录表，每个文件占一个目录项，目录项中有含有文件名、扩展名、文件长度等信息。此外，为了描述目录项是空闲还是被占用，增设一个“状态位”，如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-31 145637.png" alt="屏幕截图 2023-12-31 145637" style="zoom: 33%;" />

单级目录固然是简单的，但是其显然不适合多道系统的情况，因为其具有查找速度慢、不允许文件重名的缺点。

​	**两级目录**基本上克服了单级目录的缺点，可以为每个用户建立一个单独的用户文件目录 $UFD$，系统中再建立一个主文件目录 $MFD$。相对而言，其优点表现为：检索快；不同的用户目录中文件可以同名；不同用户可以用不同的文件名访问同一个文件。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片4.jpg" alt="图片4" style="zoom:50%;" />

​	**多级目录**又叫树形结构目录，是现代操作系统中最通用的目录组织形式，它采用一种有向无循环图的表示方法。主目录在这里叫根目录，一个文件目录只能有一个根目录；每个文件、每个子目录只能有一个父目录；数据文件为叶子结点。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片5.jpg" alt="图片5" style="zoom: 50%;" />

​	**4.2.3  目录查询** 查询目录是为了访问文件。首先，找到FCB或索引结点，根据物理地址换算出文件在外存上的物理位置，启动磁盘把数据读入内存。目录查询的方法有两种：线性查找和哈希方法。

​	**线性查找法**也就是顺序查找法。是根据路径在树形目录中寻找指定文件的方法，这是一个逐层检索并逐层报告检索结果的过程。例如，用户给定的文件路径是 $usr/ast/src$，则系统先读入第一个文件分量 $ust$，寻找根目录（或当前目录）下的匹配项。假设得到的匹配项索引号为 $6$，从 $6$ 号结点中找到 $ast$ 目录存在 $132$ 号外存物理块中，则将其读入内存。继续上述过程，直至找到文件 $src$。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片6.png" alt="图片6" style="zoom:50%;" />

**哈希方法**基于哈希索引文件目录。根据用户的文件名，获取文件目录的索引值，并利用该索引值到目录中去查找。

<div style="page-break-after:always"></div>

## 4.3  文件的共享

​	**定义4.3.1（文件共享）** 允许多个用户（进程）共享同一份文件。这样，在系统中只需要保留该共享文件的一份副本，否则就意味着每个用户都要有该文件的一个副本。我们主要讲两种有关文件共享的方式。一种是基于索引结点的文件共享，一种是利用符号链接的文件共享。

​	**4.3.2  基于索引结点的文件共享** 我们知道，文件目录中只设置文件名及指向相应索引结点的指针，而文件的物理地址、文件长度及其它的文件属性等信息只存放在索引结点中。为此，我们可以在索引结点上设置“链接计数”这一属性，每共享一个新用户（进程），链接计数$++$，如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片7.jpg" alt="图片7" style="zoom:50%;" />

这时就涉及到一个文件删除权限的问题。即：共享之后，文件主人并不是想删除该文件就能删除该文件，因为共享者可能还在文件上做一些写操作，强制的删除会使共享者的指针悬空。

​	**4.3.3  利用符号链接的文件共享** 符号链接是一种 $Link$ 类型的文件。当有新用户想要共享某一文件时，由系统创建一个同名的$Link$ 类型新文件，新文件中包含该文件的路径名。操作系统会对 $Link$ 类型文件的操作进行截获，并解释执行。这一共享方案的优点是，只有文件主才拥有指向文件索引结点的指针，而且能够链接任何地方的文件；缺点则是，按路径查找的访问开销大，而且每增加一条链接，同一个共享文件就要多一个文件名，这可能会在拷贝粘贴时出现额外开销。

​	***4.3.4  $Linux$ 中的文件共享**  $Linux$ 中的文件共享方式有两种，一种叫硬链接$(Hard\ Link)$，本质上就是基于索引结点的文件共享；一种叫软链接$(Soft\ Link)$，本质上就是利用符号链接的文件共享。

<div style="page-break-after:always"></div>

## 4.4  文件的外存组织方式

​	在前文，我们已经了解了文件的逻辑结构。接下来我们来看一看文件的物理结构，即：文件是如何在外存中存放的。目前常用的外存组织方式有：连续分配、链接分配、索引分配。

​	**4.4.1  连续分配** 这是最简单的一种分配方式，即为每一个文件分配一组相邻接的盘块，在物理上形成了顺序文件结构。根据我们在存储器管理一章学到的知识，这种分配方式必然导致外碎片的出现，这时我们可以使用紧凑的方法来解决。
​	在连续分配方式中，一个文件的目录项应当包含始址和长度。下图是一个使用位示图方式表示连续分配的示例。

<img src="C:\Users\21117\Pictures\Typora\图片8.jpg" alt="图片8" style="zoom:50%;" />

连续分配的优点是：实现简单，速度快；缺点是：不能灵活地删除和插入记录、很难实现文件的动态增长。

​	**4.4.2  链接分配** 这是一种离散的分配方式，能适应文件的动态增长。链接分配的“链接”有显式链接和隐式链接两种。
​	隐式链接的思想是，目录项中存储文件的首块和末块，在每一块中存放指向下一块位置的指针，末块的指针为 $-1$。如下图所示。这种方案只适合顺序访问，不适合跳跃性的访问。

<img src="C:\Users\21117\Pictures\Typora\图片9.png" alt="图片9" style="zoom: 17%;" />

​	显式链接的思想是，把用于链接文件各物理块的指针，显式地存放在内存的一张“链接表”中，查找在内存中进行。这张表就是文件分配表 $FAT$。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片10.png" alt="图片10" style="zoom:20%;" />

显式链接的问题是，$FAT$ 要占用很大的内存空间。因为块号是随机分布在 $FAT$ 中的，所以想要查出块，就必须把整个 $FAT$ 调入内存。这就会引起不恰当的开销。

​	**4.4.3  索引分配** 我们想到，在打开某个文件时，只需要把它所占用的盘块调入内存即可，而不用调用 $FAT$ 表。为此，应当设置一个索引，把某个文件所占用的盘块号集中放在一起。这就是索引分配的基本原理。我们为每个文件分配一个索引表，把分配给它的所有块号都写在里面。单级索引分配方案的位示图如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片11.png" alt="图片11" style="zoom:14%;" />



​	对于较大的文件，一个索引块装不下它的全部块号，需要多个索引块。这时，我们就可以使用多级索引分配方案，将这些索引块挂到一个二级索引上。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片12.png" alt="图片12" style="zoom:16%;" />

<div style="page-break-after:always"></div>

## 4.5  空闲存储空间的管理

​	在 4.4 中，我们认识到了文件在外存中的组织方式。实际上，在为文件分配盘块时，为了实现快速分配，我们要使用一种数据结构将那些没有存入文件的盘块组织起来。类似于我们在动态分配内存时，需要将空闲的分区做一个空闲分区链表。这就是本节课要讲述的空闲存储空间的管理。

​	**4.5.1  空闲表法** 这是对于**连续分配**方式的存储空间管理形式。在空闲表的每个表项中，我们需要存入空闲区的起始盘块号和盘块数。分配时，自然可以使用首次适应算法（First Fit）和最佳适应算法（Best Fit）；回收时，要考虑相邻分区的合并。

​	**4.5.2  位示图法** 用二进制一位来表示磁盘中一个盘块的使用情况。如用 $0$ 表示空闲，$1$ 表示占用。设置一个 $m \times n$ 的数组对一维磁盘实现二维组织。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片c.png" alt="图片c" style="zoom:50%;" />

​	在分配盘块时，顺序扫描位示图，找到一个或一组值为 $0$ 的位，然后将找到的位转换成与之相对应的盘块号：$b = n(i - 1) + j$，其中 $n$ 为每行的列数，$i$ 为行下标，$j$ 为列下标。
​	在回收盘块时，将回收的盘块号转换成位示图中的行号和列号，使用以下公式：
$$
i = \frac{b - 1}{n} + 1, \ \ \ \ \  j = (b - 1) \% n + 1
$$
​	使用位示图的优点是，从位示图中很容易找到一个或一组相邻接的空闲盘块、位示图占用空间小，可常驻内存。

​	**4.5.3  成组链接法** 这一分配方式把空闲块分为若干组，如下图所示。

<img src="C:\Users\21117\Pictures\Typora\图片4c.png" alt="图片4c" style="zoom:15%;" />

​	我们来稍微解释一下上面这张图。首先，左边的“空闲盘块号栈”是唯一进入内存的空闲块组，$S.free = 100$ 这表示该组有 $100$ 个空闲块，包括 $99$ 个实际空闲块$(1 \sim 99号位置)$和 $1$ 个指向下一空闲块组的块号指针$(0号位置的值为300)$；编号为 $300$ 的块就是被它指向的下一空闲盘块组，同样存放了 $99$ 个空闲块$(301\sim 399)$和下一空闲块组的块号指针$(400)$。当没有下一组盘块时，指针位写 $0$，如编号 $7900$ 的那个块所示。
​	在分配盘块时，先读 $S.free$，如果不为 $1$，则直接调用一个空闲块分配；如果为 $1$，说明本次分配之后栈内无空闲盘块，这时就要先读入下一空闲块组，再分配本次那个应该分配的盘块。当 $S.free = 0$ 时，系统无可分配的盘块。
​	在回收盘块时，若内存块组不满 $100$ 块，则只需将回收块的块号填入栈的栈顶，$S.free++$；如果内存块组已满，则必须先将它写回外存，然后将空闲盘块数 $1$ 和回收块号记入内存块组中。

<div style="page-break-after:always"></div>

## 习题

1. 设一个文件系统有 $10000$ 个记录。按照顺序查找，平均需要查找多少次？如果将其组织为索引顺序文件，设 $100$ 个记录一组，索引表的找法设为顺序法，则平均查找次数又为多少？

2. 某文件系统采取多级索引的方式组织文件的数据存放，假定在文件的 $i\_node$ 中设有 $13$ 个地址项，其中直接索引 $10$ 项，一次间接索引项 $1$ 项，二次间接索引项 $1$ 项，三次间接索引项 $1$ 项，数据块的大小为 $4KB$，磁盘地址用 $4B$ 表示，试问：这个文件系统允许的最大文件长度是多少？

3. 假定磁盘块的大小为 $1KB$，对于 $540MB$ 的硬盘，其文件分配表 $FAT$ 最少需要占用多少存储空间?

   


<div style="page-break-after:always"></div>

# 第五章  I/O 系统

## 5.1  I/O 系统及其层次结构

​	在我们介绍操作系统对 I/O 的管理之前，我们先使用一些较为主观的例子看一看什么是 I/O 系统。

​	我们使用计算机的时候，经常会使用外接的鼠标和键盘，这就是外接的输入设备。以键盘为例，键盘的输入是把我们的按键输入信号转为字符对应的 ASCII 码，然后通过数据线输入到计算机中。如果你了解过计算机底层的数字逻辑的话，你应该很快就能想到：上述过程是可以通过逻辑电路和数据选择器来实现的。那么这就是输入设备（Input 设备，I 设备）。与之相对，连接到电脑的打印机显然就是一种输出设备（Output 设备，O 设备）。

​	**5.1.1  I/O 系统的基本功能**  I/O 系统管理的主要对象是 I/O 设备和相应的设备控制器。其最主要的任务是，完成用户提出的 I/O 请求，提高 I/O 速率，以及提高设备的利用率，并能为更高层的进程方便地使用这些设备提供手段。其基本功能分为以下六个方面：

​	**隐藏物理设备的细节** 不同的 I/O 设备需要不同的命令和参数，如果要求程序员或用户编写直接面向这些设备的程序是极其困难的。因此，I/O 系统必须通过对设备加以适当的抽象，以隐藏掉物理设备的实现细节，仅向上层进程提供少量的、抽象的读写命令。
​	**与设备的无关性** 在隐藏物理设备的细节的基础上，使用户不仅可以使用抽象的I/O命令，还可使用抽象的逻辑设备名来使用设备。
​	**提高处理机和 I/O 设备的利用率** 尽可能地让处理机和 I/O 设备并行操作，以提高它们的利用率。
​	**对 I/O 设备进行控制** 这显然是 I/O 系统最基础的功能。一般有四种控制方式：轮询、中断、直接存储器访问 DMA、I/O 通道。
​	**确保对设备的正确共享** 区分独占设备和共享设备。
​	**错误处理** 对于临时性错误，通过重试操作来纠正；对于持久性错误，向上层报告。

​	**5.1.2  I/O 系统的层次结构** 操作系统中的 I/O 系统是具有层次结构的。自底向上，分别是：中断处理程序、设备驱动程序、与设备无关的 I/O 软件、用户层的 I/O 软件。如下图。本章节中对 I/O系统的介绍就是基于上述层次结构的。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-31 175240.png" alt="屏幕截图 2023-12-31 175240" style="zoom:33%;" />

I/O 系统的一种层次视图如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-31 175625.png" alt="屏幕截图 2023-12-31 175625" style="zoom: 30%;" />

I/O 系统各层次之间的关系和计算机网络的分层理念相类似。下层向上层提供服务，上层通过调用下层的接口完成对应的服务。在低层与高层之间的接口中，根据设备类型的不同，又进一步分为若干个接口。如块设备接口、流设备接口和网络接口等。

​	**块设备接口**是设备管理程序和高层的接口，用于大部分磁盘存储器和光盘存储器的输入输出。
​	**流设备接口**又称为字符设备接口，是流设备管理程序与高层之间的接口。它反映了大部分字符设备的本质特征，用于控制字符设备的输入或输出。
​	**网络接口**把计算机连接到网络上，从而实现计算机间的资源共享和信息传递。

<div style="page-break-after:always"></div>

## 5.2  I/O 设备和设备控制器

​	一般地，I/O 设备由执行 I/O 操作的机械和执行控制 I/O 的电子部件组成。通常将这两部分分开，执行I/O操作的机械
部分就是一般的 **I/O 设备**，而执行控制 I/O 的电子部件则称为**设备控制器**或适配器(adapter)。

​	**5.2.1  I/O 设备** 按使用特性分，I/O 设备可以分为存储设备与输入输出设备；按传输速率分，I/O 设备可以分为低速设备（键盘、鼠标等）、中速设备（打印机等）、高速设备（磁带机、光盘机等）；按设备的共享属性分，I/O 设备可以分为独占设备、共享设备、虚拟设备。

​	**5.2.2  I/O 设备与设备控制器间的接口** 通常，设备并不是直接与 CPU 进行通信，而是与设备控制器通信。因此，在 I/O 设备中含有与设备控制器间的接口。在该接口中有三种类型的信号：数据信号、控制信号、状态信号，各对应一条信号线。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-31 180822.png" alt="屏幕截图 2023-12-31 180822" style="zoom: 25%;" />

其中，数据信号线传送数据信号，对输入而言，是由设备发送给设备控制器的；对输出而言，是由设备控制器所接收的比特流。控制信号线传送设备控制器向 I/O 设备发送的控制信号，规定设备将要执行的操作。状态信号线用于传送指示设备的当前状态的信号。

​	**5.2.3  设备控制器** 设备控制器位于 CPU 与设备之间，它既要与 CPU 通信，又要与设备通信。其基本功能有：

~~~
接收和识别命令;
数据交换;
标识和报告设备的状态;
地址识别;
数据缓冲;
差错控制.
~~~

现有的大多数控制器都是由以下三部分组成：设备控制器与处理机的接口、设备控制器与设备的接口、 I/O 逻辑。其大概组成如下图所示。

<img src="C:\Users\21117\Pictures\Typora\30adcbef76094b369e116deea3cc7cd98d109d35.webp" alt="30adcbef76094b369e116deea3cc7cd98d109d35" style="zoom:50%;" />

​	**5.2.4  内存映像 I/O** 如上图所示，我们可以看到的是，在设备控制器执行命令的时候，需要调用属于自己的寄存器中的数据。如果我们不做处理，这很有可能要在计算机中引入一些新的指令。为了避免扩充计算机的指令系统，有人提出了内存映像 I/O 的处理方式，即：在编址上不再区分内存单元地址和设备控制器地址，都采用 $p$，当 $p$ 在一个范围内时表示内存地址；当 $p$ 在上述范围的补集中时表示设备控制器的寄存器地址。

​	**5.2.5  I/O 通道** 虽然在 CPU 和 I/O 设备之间增加了设备控制器之后，可以大大减少 CPU 对 I/O 的干预，但是当主机的外设很多时，CPU 的负担依然很重。为此，在 CPU 和设备控制器之间有增设了 I/O 通道，主要目的是：通过让通道处理一些原来由 CPU 承担的 I/O 任务，建立相对独立的 I/O 操作。I/O 通道的类型有字节多路通道、数组选择通道和数组多路通道三种。
​	I/O 通道存在一个瓶颈问题，即：系统中所设置的通道数量一般都会很少（因为通道贵），这往往会造成整个系统吞吐率下降。一种解决方案是，增加设备到主机间的通路而不增加通道。如下图所示，就是单通路 I/O 系统变为多通路 I/O 系统。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-31 183308.png" alt="屏幕截图 2023-12-31 183308" style="zoom: 28%;" /><img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-31 183334.png" alt="屏幕截图 2023-12-31 183334" style="zoom: 30%;" />

<div style="page-break-after:always"></div>

## 5.3  中断机构、中断处理程序

​	中断在操作系统中有着特殊重要的地位，它是多道程序得以实现的基础，中断也是设备管理的基础。中断处理程序时 I/O 系统中最低的一层，也是整个 I/O 系统的基础。

​	**定义5.3.1（中断）** 计算机在执行期间，系统内发生任何非寻常的或非预期的急需处理事件，使得CPU暂时停止当前正在执行的程序而转去执行相应的事件处理程序。处理完毕后，又返回原来被中断处继续执行或调度新的进程执行的过程。引起中断发生的事件被称为**中断源** ，中断源向 CPU 发生的请求处理信号被称为**中断请求**，CPU 收到中断请求后转相应的事件处理程序称为**中断响应**。容易理解的是，中断请求是有优先级的。几种比较常见的中断具有这样的优先级排列：磁盘的中断请求 > 打印机的中断请求 > 用户的中断请求。
​	根据中断产生的条件，可以把中断分为内中断和外中断。**内中断**也叫**陷入**(trap)，指在处理机内部产生的中断。它包括程序运算引起的各种错误，如地址非法、校验错、页面失效、存取访问控制错、算术操作溢出、数据格式非法、除数为零、非法指令、用户程序执行特权指令、分时系统中的时间片中断等。**外中断**指来自处理机外部的中断。包括 I/O 设备发出的 I/O 中断、外部信号中断(如 Ctrl + C 中断)、时钟中断以及调试程序中设置的断点等引起的调试中断等。

​	**5.3.2  中断向量表** 每种设备配以相应的中断处理程序，而中断服务程序的入口地址称为**中断向量**。把所有的中断向量集中起来，按中断类型号从小到大的顺序存放到存储器的某一区域内，这个存放中断向量的存储区叫做中断向量表，即中断服务程序的入口地址表。当 I/O 设备发来中断请求信号时，中断控制器确定该请求的中断号，根据该设备的中断号去查找中断向量表，从中取得该设备中断处理程序的入口地址，转入中断处理程序执行。中断向量表一般使用**数组**的组织形式。

​	我们拍脑袋想一个问题：当处理器正在处理一个中断时又来了一个中断该怎么处理呢？很显然有两种处理方式：一种是不管它，即：只要正在处理中断，就不再管其它的中断，这叫**屏蔽中断**；一种是按照优先级处理，允许高优先级中断抢占低优先级的中断，这就叫**嵌套中断**。

​	**5.3.3  中断处理程序** 中断处理程序的处理过程包括如下步骤：测定是否有未响应的中断信号、保护被中断进程的 CPU 现场、转入相应的设备处理程序、中断处理、恢复 CPU 现场并退出中断。其流程图如下所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-31 190206.png" alt="屏幕截图 2023-12-31 190206" style="zoom: 33%;" />

<div style="page-break-after:always"></div>

## 5.4  设备驱动程序

​	**5.4.1  设备驱动程序的功能**  设备驱动程序一般具有如下功能：

```
接收由与设备无关的软件发来的命令和参数，并将命令中的抽象要求转换为与设备相关的低层操作序列。
检查用户I/O请求的合法性，了解I/O设备的工作状态，传递与I/O设备操作有关的参数，设置设备的工作方式。
发出I/O命令，如果设备空闲，便立即启动I/O设备，完成I/O操作；如果设备忙碌，则将请求者的请求块挂在设备队列上等待。
及时响应由设备控制器发来的中断请求，并根据其中断类型，调用相应的中断处理程序进行处理。
```

​	**5.4.2  I/O的控制方式**  设备驱动程序对 I/O 的控制方式如下图所示。图中的“字”指的是一个字符，也就是说，每读入一个字符，就要做一个状态的检测：	

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-05 133518.png" alt="屏幕截图 2023-12-05 133518" style="zoom: 33%;" />

​	（a）**程序I/O方式**和（b）**中断驱动方式**的区别就在于，（b）通过允许CPU做其他事大大提高了CPU的利用率。形象地说，这很像一个幼儿园老师给小朋友发糖的过程。（a）中，老师一直在给同一个小朋友发糖，并不断地做询问“是否吃完了”，在获得了“吃完了”的回答之后立刻发给他下一块；（b）中，老师在给小朋友发完糖后，不再不断地询问，而是去做别的事情，等到该小朋友吃完了这块糖，就会自动向老师发送请求索要下一块。

​	在这样的前提下，我们思考：能不能不要每读入一个字符就做中断检测而是在读入一定内容之后再做检测呢？为此，我们有了（c）**DMA方式**。DMA方式的读入以块为单位，每读入一个块，检测是否有中断请求。如果有则中断。这样一来，I/O占用CPU的时间又会进一步减少。继续使用发糖的比喻，这个过程就像老师一次给一个小朋友发五块糖，然后再去做别的事情，等这个小朋友吃完了这五块糖再索要下五块。

​	另外，还有一种 **I/O 通道的控制方式**。它可进一步减少 CPU 的干预，即把对一个数据块的读(或写)为单位的干预，减少为对一组数据块的读(或写)及有关的控制和管理为单位的干预。同时，又可实现 CPU、通道和 I/O 设备三者的并行操作，从而更有效地提高整个系统的资源利用率。



<div style="page-break-after:always"></div>

## 5.5  与设备无关的 I/O 软件

​	**定义5.5.1（设备独立性）** 设备独立性也称设备无关性。其基本含义是：应用程序独立于具体使用的物理设备，使用**逻辑设备名**来请求某类设备，系统在执行时，则使用该类设备的物理设备名。驱动程序是一个与设备紧密相关的软件，为实现设备独立性，必须再在驱动程序之上设置一层**设备独立性软件**。

​	**5.5.2  与设备无关的软件**  在早期OS中，应用程序在使用 I/O 设备时，都使用设备的物理名称，这使应用程序与系统中的物理设备直接相关；后来，引入了逻辑设备名，为了实现与设备的无关性而引入了逻辑设备和物理设备两个概念。逻辑设备名称到物理设备名称的转换。在应用程序中，用逻辑设备名称使用设备虽然方便了用户，但系统却只识别物理设备名称，因此在实际执行时，还必须使用物理名称。为此，在系统中，必须具有将逻辑设备名称转换为某物理设备名称的功能。实现与设备无关性的软件是 I/O 系统的高层软件。

​	**定义5.5.3（设备分配）** 在多道程序环境下，设备必须由系统分配，而不能由用户的程序分配。每当进程向系统提出I/O请求时，设备分配程序按照一定的分配策略，把其所需的设备及其有关资源（如缓冲区、控制器和通道）分配给该进程。在分配设备时还必须考虑系统的安全性，避免发生死锁。

​	**设备控制表**（DCT）是设备分配中需要用到的数据结构之一。内容包括设备类型 $dtype$、设备标识符 $dID$、设备状态（是否等待、是否忙）、指向控制器表的指针、设备队列的队首指针等。另外，设备分配中需要的数据结构可能还有控制器控制表（COCT）、通道控制表（CHCT）、系统设备表（SDT）等。后三种数据结构的结构大致如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-05 144458.png" alt="屏幕截图 2023-12-05 144458" style="zoom:33%;" />

​	那么我们不禁要问，在分配设备的时候要考虑哪些因素呢？首先当然是设备的**固有属性**，即这一设备是独占设备、共享设备还是虚拟设备。显然，如果该设备是独占设备，我们可能就需要使用信号量等方式实现互斥访问。其次就是设备分配的**算法**，算法一般包括 FIFO 算法和优先级高者优先算法等。最后是分配中的**安全性问题**，即我们的分配是否会导致死锁的问题。

​	为了实现设备的无关性，应用程序请求使用 I/O 设备时，采用的是逻辑设备名，但是，系统只能识别物理设备名。因此，需要配置一张**逻辑设备表**（LUT），用于将逻辑设备名映射为物理设备名。LUT 的设置有两种方式，第一种方式则是为整个系统设置一张 LUT，这就不允许表中有相同的逻辑设备名，这在多用户环境下基本是不可能的；第二种方式则是为每个用户都设置一张 LUT，并将其放入 PCB 中。

<div style="page-break-after:always"></div>

## 5.6  用户层的 I/O 软件

​	**5.6.1  系统调用与库函数**  应用程序通过系统调用，间接调用OS中的I/O过程，对I/O设备进行操作；用户程序通过调用对应的库函数来使用系统调用，这些库函数与系统调用连接在一起。

​	**5.6.2  Spooling 技术**  在多道程序系统中，利用一道程序来模拟脱机输入时的外围控制机的功能，把低速I/O设备上的数据传送到高速磁盘上；再利用另一道程序模拟脱机输出时外围控制机的功能，把数据从磁盘传送到低速输出设备上。这样，在主机的直接控制下，实现脱机输入输出功能。此时的外围操作与CPU对数据的处理同时进行，把这种在联机情况下实现的同时外围操作称为 **Spooling 技术**(Simultaneous Peripheral Operation On-Line)，或称为**假脱机技术**。通过Spooling技术可以将一台独占的物理设备虚拟为多台逻辑设备，从而允许多个用户（进程）共享。其工作原理大致如下图所示：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-05 153050.png" alt="屏幕截图 2023-12-05 153050" style="zoom:33%;" />

简而言之，Spooling 技术使用了一个输入井来预存输入的数据。当数据进入输入井时，在用户的视角来看就是作业输入完成了。随后操作系统会使用井管理程序调度输入井中的内容将其调入内存。这会给用户一个自己独占内存的错觉。输出的部分同理。Spooling 技术是使用空间换时间的好例子。

​	接下来，我们来看看一台打印机如何通过 Spooling 技术虚拟为多台打印机，即“假脱机打印机系统”。如下图所示。图中硬盘中的空盘块队列和满盘块队列就是我们所说的输入井和输出井。

  

该系统接收到用户进程的打印输出请求之后，会做以下事情：
~~~
由假脱机管理进程在输出井中为之申请一个空闲磁盘块区，并将要打印的数据送入其中。
假脱机管理进程为用户进程申请一张空白用户请求打印表，并将用户的打印要求填入其中，再将该表挂到假脱机文件队列上。
打印机空闲时，假脱机打印进程将从假脱机文件队列的队首取出一张请求打印表，再从输出井把数据送到内存缓冲区，启动打印机打印输出。打印完后，假脱机打印进程再次检查请求打印队列，从而实现循环。
~~~

​	**5.6.3  缓冲管理**  我们可以看到的是，在上述模型中使用了“缓冲”的概念。类似地，在其他 I/O 设备上也存在缓冲区的设置，在操作系统中存在一种**缓冲管理系统**。引入缓冲的根本原因，就是缓和 CPU 与 I/O 设备之间速度不匹配的问题。

​	在**单缓冲**（只有一个缓冲区）的情况下，每当用户进程发出一条 I/O 请求时，操作系统便在主存中为其分配一个缓冲区。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-05 155902.png" alt="屏幕截图 2023-12-05 155902" style="zoom:33%;" />

上图告诉我们，用户的处理和 I/O 设备的输入是可以并行执行的。所以第 $i$ 个数据块的处理时间为：
$$
\max(C_i,\ T_i) + M_i
$$
需要注意的是，**最后一次是不可并行的**。

​	由于缓冲区是共享资源，生产者与消费者在使用缓冲区时必须互斥。如果消费者尚未取走缓冲区中的数据，即使生产者又生产出新的数据，也无法将它送入缓冲区，生产者等待。如果为生产者与消费者设置了两个缓冲区，便能解决这一问题。这就是**双缓冲**，也叫**缓冲对换**，即：输入时先往第一缓冲区中输入数据，满后再往第二缓冲区中输入数据，此时，操作系统就可以从第一缓冲区中取走数据。图示如下：

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231212133751679.png" alt="image-20231212133751679" style="zoom:33%;" />

​	对于上$(b)$图，我们可以看到，它和单缓冲不同的是，缓冲的输入时操作 $T$ 可以流水执行。所以第 $i$ 个数据块的处理时间为：
$$
\max(C_i,\ T_i) + M_i
$$
假定这是一个输入为主体的操作，即输入时间（$T$）远大于传送时间（$M$），则第 $i$ 个数据块的处理时间为：
$$
max(C_i,\ T_i)
$$
​	我们来思考这样一个问题：当生产者和消费者使用缓冲区时速度不匹配时，即生产者生产的速度太快，而消费者消费的速度太慢，这样的话就会造成缓冲区内的堆积。为了解决这样的问题，我们引入一种新的缓冲管理方式——**循环缓冲**。如下图所示。

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231212134722821.png" alt="image-20231212134722821" style="zoom:33%;" />

​	上图中的 $R$ 表示空的缓冲区，$G$ 表示满的缓冲区，$C$ 表示正在工作的缓冲区（消费者正在取或生产者正在放）。这里可能涉及到这样的同步问题：

~~~
Nexti 追赶上 Nextg 时，说明生产速度快（空的少了，满的多了，Nexti 才会移动）。系统受计算限制；
Nextg 追赶上 Nexti 时，说明消费速度快（满的少了，空的多了，Nextg 才会移动）。系统受I/O限制。
~~~

​	**定义（缓冲池）** 既可用于输入又可用于输出的公用缓冲区域。其中至少包含三种类型的缓冲区队列：

~~~
空闲缓冲区队列（emq）；
装满输入数据的缓冲区队列（inq）；
装满输出数据的缓冲区队列（outq）。
~~~

另外，还有四种工作缓冲区：

~~~
用于收容输入数据的工作缓冲区 hin;
用于提取输入数据的工作缓冲区 sin；
用于收容输出数据的工作缓冲区 hout；
用于提取输出数据的工作缓冲区 sout。
~~~

其工作方式的图示如下：

<img src="C:\Users\21117\AppData\Roaming\Typora\typora-user-images\image-20231212140245843.png" alt="image-20231212140245843" style="zoom:33%;" />

我们来使用伪代码看一下这四种操作。其中，这四种操作的基操作为取缓冲区的 $getbuf$ 和放缓冲区的 $putbuf$。

~~~
收容输入：
	hin = getbuf(emq);
	input to buf;
	putbuf(inq, hin);
提取输入：
	sin = getbuf(inq);
	input to client;
	putbuf(emq, sin);
收容输出：
	hout = getbuf(emq);
	output to buf;
	putbuf(outq, hout);
提取输出：
	sout = getbuf(outq);
	output to client;
	putbuf(emq, sout);
~~~

<div style="page-break-after:always"></div>

## 5.7  磁盘存储管理

​	首先，希望你知道的是，磁盘确实是“盘”。如下图所示。

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-12 143550.png" alt="屏幕截图 2023-12-12 143550" style="zoom:33%;" />

​	**5.7.1  磁盘的基本结构** 磁盘上能存储的物理块数目由盘面数、磁道数、扇区数决定。一个盘块有两个盘面，一个盘面上有很多磁道，一个磁道上包括许多扇区。磁盘的地址由柱面号、盘面号、扇区号确定。每个盘面一个磁头，装在磁臂中，磁头能够移动寻找磁道。

​	当一个磁盘刚开始使用的时候，要做物理格式化和扇区的划分，通俗一点，可以理解为把 $Windows$ 的磁盘划分为 C 盘、D 盘和 E 盘，把每个分区的起始扇区和大小记录在分区表中；划分结束之后，要对磁盘做逻辑格式化，即创建文件系统的过程；最后才是在 C 盘内安装操作系统的过程。

​	**定义5.7.2（自举程序）** 一个初始化程序，用来初始化 CPU、寄存器、设备控制器和内存等。自举程序通常存放在 ROM 中，通常地，它会引导你去到磁盘的第一个扇区读主引导记录（MBR），从而运行主引导程序，完成磁盘的分区，然后启动操作系统。

​	**5.7.3  磁盘访问** 磁盘访问的时间与以下三类时间有关：
 	1. 寻道时间 $T_s$：把磁臂（磁头）移动到指定磁道上所经历的时间，包含启动磁臂和磁头移动 $n$ 条磁道所花费的时间。$T_s = m \times n + s$.
 	2. 旋转延迟时间 $T_r$：指定扇区移动到磁头下面所经历的时间。与盘面的旋转速度有关。平均值为：$T_r = \frac{1}{2r}，$其中 $r$ 是磁盘每秒的转数。
 	3. 传输时间 $T_t$：把数据从磁盘读出或向磁盘写入数据所经历的时间。与旋转速度和一次读写的数据量有关。$T_t = \frac{b}{rN}，$其中 $b$ 为数据量，$N$ 为磁道的分布情况。

综上，总的访问时间为：
$$
T = T_s + \frac{1}{2r} + \frac{b}{rN}.
$$
​	其中，只要磁盘确定了，$T_r、T_t$ 都是确定的。因此，我们要降低访问时间，就是要降低其寻道时间 $T_s$。实现的算法有以下几种：

​	**先来先服务（FCFS）算法和最短寻道时间优先（SSTF）算法**

​	**先来先服务**显然是根据进程请求访问磁盘的先后次序做调度。优点是公平、简单，缺点则是平均寻道长度太长，只适用于磁盘请求比较少的场合。
​	**最短寻道时间优先**则是一种贪心算法。其贪心选择属性体现为：选择要求访问的磁道与当前磁头所在的磁道距离最近的进程（磁盘请求），使每次的寻道时间最短。该算法固然会降低平均寻道长度，但是可能导致饥饿现象。

​	我们来看一个具体的例子看一看上面的两种算法：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-12 150442.png" alt="屏幕截图 2023-12-12 150442" style="zoom: 25%;" />

​	**扫描（SCAN）算法与循环扫描（CSCAN）算法**

​	**扫描算法**借鉴了 SSTF 算法中的一些思路。在该算法中，磁头每次只作单方向移动，在扫描到最边缘的磁道之后，磁头再作反向移动，也即：下一次待访问的磁道只能在此磁头移动的前方。由于这很像电梯运行的方式，故又称**电梯算法**。这一算法消除了饥饿现象。
​	**循环扫描算法**是对扫描算法的改进。在这一算法中，磁头只作由内向外的单方向扫描，到达最外侧需要访问的磁道后，则返回到另一个的最外端进行下一轮扫描。改进了对于边缘区磁道访问的不公平性。

​	我们来看一个具体的例子看一看上面的两种算法：

<img src="C:\Users\21117\Pictures\Typora\屏幕截图 2023-12-12 152855.png" alt="屏幕截图 2023-12-12 152855" style="zoom:25%;" />

我们可以看到，CSCAN算法的平均寻道长度会比SCAN算法的平均寻道长度要长。这也很容易理解。

​	**分治算法**

​	我们当然还可以使用分治的思想解决问题。有一种算法叫**N-Step-Scan算法**，将磁盘请求队列分成若干个长度为 $N$ 的子队列，磁盘调度将按 FCFS 算法依次处理这些子队列。而每处理一个队列时又是按 SCAN 算法，对一个队列处理完后，再处理其他队列。
​	另外，上面这一算法可以简化为**FSCAN算法**。FSCAN只将磁盘请求队列分成两个子队列。一个是由当前所有请求磁盘 I/O 的进程形成的队列，由磁盘调度按 SCAN 算法进行处理。在扫描期间，将新出现的所有请求磁盘 I/O 的进程，放入另一个等待处理的请求队列。

<div style="page-break-after:always"></div>

## 习题

1. 某文件占 10 个磁盘块，现要把该文件磁盘块逐个读入主存缓冲区，并送到用户区进行分析。假设一个缓冲区与一个磁盘块大小相同，把一个磁盘块读入缓冲区的时间为 $100\mu s$，将缓冲区的数据传送到用户区的时间为 $50\mu s$，CPU对一块数据进行分析的时间为 $50\mu s$。请问，在单缓冲区和双缓冲区结构下，读入并分析完该文件的时间分别是多少？
2. 一个磁盘的转速为 $7200r/min$，每个磁道有 $160$ 个扇区，每个扇区有 $512B$，那么在理想的情况下，其数据传输率为多少？
3. 某硬盘有 $200$ 个磁道（最外侧磁道号为 0 ），磁道访问请求序列为 $130,42,180,15,199$。当前磁头位于第 $58$ 号磁道，并从外侧向内侧移动。按照SCAN算法处理完上述请求之后，磁头移过的磁道数是多少？
4. 某计算机系统中的磁盘有 $300$ 个柱面，每个柱面有 $10$ 个磁道，每个磁道有 $200$ 个扇区，扇区大小为 $512B$。文件系统中的每个簇包含 $2$ 个扇区。回答下列问题：
   1. 磁盘的容量是多少？
   2. 假设磁头在 $85$ 号柱面上，此时有 $4$ 个磁盘访问请求，簇号分别是 $100260,60005,101660,110560.$ 如果采用 SSTF 算法，则系统访问簇的先后次序是什么？如果扇区数不是 $200$ 而是 $100$，先后次序又该是什么？
   3. 如果扇区数不是 $200$ 而是 $100$，那么第 $100530$ 簇在磁盘上的物理地址是什么？将簇号转换成磁盘物理地址的过程是由 I/O 系统的什么程序完成的？

<div style="page-break-after:always"></div>

# 部分习题答案与提示

**第一章习题**

1. 略

2. 

   ~~~
   // 设哲学家编号1，2，3，4，5；对应数组编号0，1，2，3，4.
   // 其左手边筷子编号与本人对应的数组编号一致。
   semaphore chopsticks[5] = {1,1,1,1,1};
   // 对于第 i 位哲学家：
   do
   {
   	if(i % 2)
   	{
   		P(chopsticks[(i + 1) % 5]);
   		P(chopsticks[i]);
   		
   		吃操作;
   		
   		V(chopsticks[i]);
   		V(chopsticks[(i + 1) % 5]);
   	}
   	
   	else
   	{
   		P(chopsticks[i]);
   		P(chopsticks[(i + 1) % 5]);
   		
   		吃操作;
   		
   		V(chopsticks[(i + 1) % 5]);
   		V(chopsticks[i]);
   	}
   }while(1);
   ~~~



**第二章习题**

1. 为每个用户的进程设置一个队列，各队列内采用 FCFS 算法，队列之间设置优先级，如果某队列中刚有进程被调度，将其队列优先级设为最低。这就是多队列调度算法。

1. $10$；$2$。

1. 略。

1. 数据结构：

   ~~~
   可用资源向量 Available 表示 m 类资源中，每一类的可用数目；
   请求矩阵 Request 是 n × m 矩阵，表示进程当前对各类资源的请求数目；
   分配矩阵 Allocation 是 n × m 矩阵，用以表示进程某一时刻的资源分配情况；
   工作向量 Work 表示系统可提供给进程继续运行的各类资源数目；
   进程向量 L 记录当前已不占有资源的进程。
   ~~~

   算法：

   1. 把不占用资源的进程记入 $L$ 表中。
   2. 从进程集合中找到一个 $Request[i]≤Work$ 的进程，做如下处理：
      1. 将其资源分配图简化，释放出资源，增加工作向量
         $Work = Work+Allocation[i]$
      2. 将它记入 $L$ 表中。
      3. 重复上述操作，若不能把所有进程都记入 $L$ 表中，则表明系统资源分配图是不可完全简化的，将发生死锁。

   



**第三章习题**

1. 1. ~~~
      5位页号，11位页内偏移。
      |-----|-----------|
      15  11 10          0
      ~~~

   2. 9 位（注意页号不需要占位）。

   3. 页表每项（块号）位数减少1位，变为8位；页内偏移位数不变。

2. $3017,6596,越界$。

3. $2^{35},2^{19}$.

4. $51137,段长越界,段号越界$。

5. $0A5C = 0000\ 10|10\ 0101\ 1100 \rightarrow 01\ 00|10\ 0101\ 1100 = 125C$.

   $103C = 0001\ 00|00\ 0011\ 1100,页号为 4，越界$。

   $1A5C = 0001\ 10|10\ 0101\ 1100, 页号为6，越界。$

6. 

   <img src="C:\Users\21117\Pictures\Typora\d591938c46cb2ef6e6a64d81cec7b82.jpg" alt="d591938c46cb2ef6e6a64d81cec7b82" style="zoom:25%;" />

   

**第四章习题**

1. $5000;\ 50 + 50 = 100.$ 

2. 一次间接索引中，一个块可以表示 $\frac{4KB}{4B} = 1K$ 个地址，所以一阶索引就是 $4MB$;

   共有 $40KB + 4MB + 4GB + 4TB$。

3. 物理块数目 $\frac{540MB}{1KB} = 540K < 2^{20}$，故 $FAT$ 的每个表目为 $20$ 位 $ = 2.5 B$。

   总大小为 $2.5B \times 540K = 1350KB$. 



**第五章习题**

1. 单缓冲：$150 \times 10 + 50 = 1550 \mu s$，
   双缓冲：$100 \times 10 + 50 + 50 = 1100 \mu s$.

2. $7200r/min = 120r/s，160 \times 512 = 80KB，80KB \times 120r/s = 9600 KB/s.$

3. $325$.

4. 1. $3 \times 10^5 KB$；
   2. 顺序为：$110560,101660,100260,60005$，
      顺序为 $100260,101660,110560,60005$；
   3. 第 $100$ 柱面、第 $5$ 盘面（磁道）、第 $30$ 扇区，驱动程序。

   

<div style="page-break-after:always"></div>

# 参考资料

1. 计算机操作系统（第四版），西安电子科技大学出版社。
2. 北京师范大学人工智能学院 肖融老师的 $PPT$。
3. $ChatGPT$：chat.openai.com.
4. [计算机操作系统_余百里的博客-CSDN博客](https://blog.csdn.net/weixin_45633206/category_10311467.html)。
5. [操作系统_蔗理苦的博客-CSDN博客](https://blog.csdn.net/zheliku/category_11341783.html)。

<div style="page-break-after:always"></div>

###  **念奴娇·操作系统结课赋**

​	计科系统，引机器会了、人间凉热。虚拟内存君莫问，都是进程轮廓。那里IO，谁家段页，裹碎平生客。预防死锁，老迪无功无过。
​	数尽王道征途，此肝此胆，潇洒宏图阔。安得两胁生素翼，奉我天鹅颜色？解透儒冠，当抛敝屣，学尔一航魄。小躯宏愿，自有灯火难落。
